{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shixiao-coder/website/blob/upload-eval-colab/nl_evals_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See [Evaluation Documentation](https://docs.google.com/document/d/1ajUTCgdGFlInAqdWJB18STaG-TX_uoN1lQFgVPSd8pg/edit?resourcekey=0-czNd7QblghzoQarG5RRMqQ&tab=t.0) for more detailed instruction + background!"
      ],
      "metadata": {
        "id": "NQaUqf8iXFkE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PdvKLgZCbfK"
      },
      "source": [
        "## (required) Run the Set Up Code\n",
        "\n",
        "**Required: [Create a Colab secret](https://drlee.io/how-to-use-secrets-in-google-colab-for-api-key-protection-a-guide-for-openai-huggingface-and-c1ec9e1277e0#:~:text=Step%2Dby%2DStep%20Guide%20to%20Using%20Colab%20Secrets) called `DC_API_KEY` with your [Data Commons API Key](https://docs.datacommons.org/api/#get-key), by clicking on the key icon in the left navigation bar and \"Add new secret\".**\n",
        "\n",
        "Includes imports, globals, utils, Pydantic model definitions, and df <-> gcs <-> sheets <-> model conversion utils.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Also includes the Scrape API code and Scoring code.\n",
        "\n",
        "### General Inputs\n",
        "This table describes the input required to define the runtime.\n",
        "\n",
        "| Parameter | Type | Default Value | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `runtime_type` | `string` | `--` | Required. This defines the runtime/environment to use. |\n",
        "| `host_website` | `string` | `--` | Optional. defines the HTTP(s) to use for given runtime_type. If left empty, it is generated from the current runtime_type. |\n",
        "\n",
        "### Local Runtime\n",
        "This part describes the setup required for local runtime.\n",
        "\n",
        "1.   mkdir eval -> cd eval\n",
        "2.   setup requirement.txt\n",
        "3.   python3 -m venv evals_runtime\n",
        "4.   source evals_runtime/bin/activate\n",
        "5.   gcloud auth application-default login --scopes=\"https://www.googleapis.com/auth/drive\",\"https://www.googleapis.com/auth/cloud-platform\"\n",
        "6.   pip3 install -q -r requirements.txt\n",
        "7.   jupyter notebook -NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0 --NotebookApp.allow_credentials=True\n",
        "8.   local runtime link with host HTTP\n",
        "\n",
        "requirement.txt packages:\n",
        "\n",
        "    requests\n",
        "    numpy\n",
        "    python-dateutil\n",
        "    pydantic\n",
        "    scikit-learn\n",
        "    pandas\n",
        "    jupyter\n",
        "    notebook\n",
        "    google-auth\n",
        "    fsspec\n",
        "    gcsfs\n",
        "    gspread\n",
        "    google-api-python-client\n",
        "    google-auth-httplib2\n",
        "    google-auth-oauthlib\n",
        "    oauth2client\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils:\n",
        "\n",
        "* `models_to_gcs_csv`(csv_path:str, models: List[BaseModel])\n",
        "\n",
        "  * Upload list of Pydantic objects to GCS bucket as a CSV.\n",
        "\n",
        "* `gcs_csv_to_sheet`(csv_path, sheet_id)\n",
        "  * Copies csv from GCS to Google Sheets - Nl Goldens Viewer by default\n",
        "\n",
        "* `gcs_csv_to_df`(csv_path, base_class: BaseModel = NlQueryResponse) -> pd.DataFrame:\n",
        "  * Read csv from GCS into Pandas Dataframe, converts dates, places, and variables columns to their respective Pydantic classes\n",
        "\n",
        "* `gcs_csv_to_pydantic_models`(csv_path:str, pydantic_model: BaseModel) -> List[BaseModel]:\n",
        "  * Read csv from GCS into a list of pydantic_model objects\n",
        "\n",
        "* `gcs_csv_to_variable_scrapes`(csv_path: str, pydantic_model: NlQueryResponse = NlGolden) -> List[VariableResponse]:\n",
        "  * Read csv from GCS into a flattened list of VariableResponse objects"
      ],
      "metadata": {
        "id": "2V3uK-vQfSHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #### Runtime Related\n",
        "runtime_type = \"local\" # @param [\"prod\",\"dev\",\"local\"]\n",
        "host_website = \"\" # @param {\"type\":\"string\"}\n",
        "\n",
        "if not host_website:\n",
        "  if runtime_type == \"prod\":\n",
        "    host_website = \"https://datacommons.org\"\n",
        "  elif runtime_type == \"dev\":\n",
        "    host_website = \"https://dev.datacommons.org\"\n",
        "  elif runtime_type == \"local\":\n",
        "    host_website = \"http://localhost:8080\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "woP1ywexRI9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9XMOxo5CLKs"
      },
      "outputs": [],
      "source": [
        "# @title ### Imports\n",
        "import statistics\n",
        "import re\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from pydantic import model_validator\n",
        "from typing import Optional, Any\n",
        "import pandas as pd\n",
        "from enum import Enum, auto\n",
        "from pydantic import BaseModel, ValidationError, field_serializer, ConfigDict\n",
        "import json\n",
        "from typing import Dict, List, Optional\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch3j7j0c_Nxt"
      },
      "outputs": [],
      "source": [
        "# @title ### Utils + Globals\n",
        "\n",
        "from datetime import datetime, date as dt_date\n",
        "\n",
        "GCS_EVAL_DIR = 'gs://datcom-nl-evals/evals'\n",
        "GCS_GOLDENS_DIR = 'gs://datcom-nl-evals/goldens'\n",
        "\n",
        "def now():\n",
        " return datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "\n",
        "def today():\n",
        " return dt_date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def scrapes_path(epoch):\n",
        "  return f'{GCS_EVAL_DIR}/{epoch}/scrape.csv'\n",
        "\n",
        "def goldens_path(epoch):\n",
        "  return f'{GCS_GOLDENS_DIR}/{epoch}.csv'\n",
        "\n",
        "def scores_path(scrape_epoch, score_epoch):\n",
        "  return f'{GCS_EVAL_DIR}/{scrape_epoch}/score_{score_epoch}.csv'\n",
        "\n",
        "def summary_path(scrape_epoch, score_epoch):\n",
        "  return f'{GCS_EVAL_DIR}/{scrape_epoch}/summary_{score_epoch}.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bopTVcaDZsFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d418599e-e802-4dc6-9e50-b0c9241f856c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-09 10:49:10--  https://raw.githubusercontent.com/clincoln8/datcom-website/refs/heads/nl-pyd/tools/nl/detection_evals/eval_models.py\r\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8002::154, 2606:50c0:8000::154, ...\r\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3565 (3.5K) [text/plain]\n",
            "Saving to: ‘eval_models.py.5’\n",
            "\n",
            "eval_models.py.5    100%[===================>]   3.48K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-09 10:49:10 (29.3 MB/s) - ‘eval_models.py.5’ saved [3565/3565]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Pydantic Models\n",
        "!wget https://raw.githubusercontent.com/clincoln8/datcom-website/refs/heads/nl-pyd/tools/nl/detection_evals/eval_models.py\n",
        "\n",
        "from eval_models import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isfrAkkCyRFx"
      },
      "outputs": [],
      "source": [
        "# @title ### DF <-> GCS <-> Sheets <-> Pydantic Utils\n",
        "import ast\n",
        "if runtime_type != \"local\":\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "!pip install google-cloud-storage --quiet\n",
        "from google.cloud import storage\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "GCS_GOLDENS_DIR = f\"gs://datcom-nl-evals/goldens\"\n",
        "\n",
        "def models_to_gcs_csv(csv_path:str, models: List[BaseModel]):\n",
        "  '''\n",
        "  This function takes a list of Pydantic models and saves them to a CSV file.\n",
        "\n",
        "  csv_path: The path where the CSV file will be saved.\n",
        "  goldens: A list of NlQueryResponse objects to be converted to CSV.\n",
        "  '''\n",
        "  golden_df = pd.DataFrame([m.model_dump(mode='json') for m in models])\n",
        "  golden_df.to_csv(csv_path, index=False)\n",
        "\n",
        "def df_to_gcs_csv(csv_path, df, pydantic_model):\n",
        "  validated_models = df_to_pydantic_models(df, pydantic_model)\n",
        "  models_to_gcs_csv(csv_path, validated_models)\n",
        "\n",
        "def df_to_pydantic_models(df, pydantic_model: BaseModel) -> List[BaseModel]:\n",
        "  '''This function converts each row of a DF into a class object.\n",
        "\n",
        "  csv_path: The path to the CSV file.\n",
        "  pydantic_model: The Model Class the csv represents.\n",
        "  Returns: A list of NlQueryResponse objects.\n",
        "  '''\n",
        "  return [pydantic_model.model_validate(record) for record in df.to_dict(orient='records')]\n",
        "\n",
        "\n",
        "def df_to_sheet_in_folder(df, sheet_name, scrape_epoch, folder_id='1-yFZ-HrhP1e-JhJ3JRbCjTmeXp78wOwn'):\n",
        "  '''This function reads a DataFrame and uploads its content to a Google Sheet,\n",
        "  handling folder and spreadsheet creation, and sheet naming based on scrape_epoch.\n",
        "\n",
        "  df: The DataFrame to upload.\n",
        "  sheet_name: The desired name for the worksheet.\n",
        "  scrape_epoch: A string representing the scrape epoch, used for spreadsheet naming.\n",
        "  folder_id: The ID of the Google Drive folder to check/create the spreadsheet in.\n",
        "  '''\n",
        "  if runtime_type != \"local\":\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "\n",
        "  import gspread\n",
        "  from gspread.exceptions import SpreadsheetNotFound, WorksheetNotFound\n",
        "\n",
        "  from google.auth import default\n",
        "\n",
        "  creds, _ = default()\n",
        "  gc = gspread.client.Client(auth=creds)\n",
        "\n",
        "  from googleapiclient.discovery import build\n",
        "  drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "  spreadsheet_name = scrape_epoch\n",
        "\n",
        "  # Check if spreadsheet already exists in folder.\n",
        "  results = drive_service.files().list(\n",
        "      q=(\n",
        "        f\"mimeType='application/vnd.google-apps.spreadsheet' and \"\n",
        "        f\"name='{spreadsheet_name}' and \"\n",
        "        f\"'{folder_id}' in parents and \"\n",
        "        f\"trashed=false\"\n",
        "    ),\n",
        "      spaces='drive',\n",
        "      driveId='0AL_se0fAJ2R9Uk9PVA',\n",
        "      includeItemsFromAllDrives=True,\n",
        "      supportsAllDrives=True,\n",
        "      corpora='drive',\n",
        "      fields='files(id, name)'\n",
        "  ).execute()\n",
        "\n",
        "  items = results.get('files', [])\n",
        "  if items:\n",
        "      spreadsheet = gc.open_by_key(items[0]['id'])\n",
        "      logging.info(f\"Spreadsheet '{spreadsheet_name}' found with ID: {spreadsheet.id}\")\n",
        "\n",
        "  else:\n",
        "    # Create a new spreadsheet in Google Drive\n",
        "    spreadsheet = gc.create(spreadsheet_name)\n",
        "    logging.info(f\"Successfully created Google Sheet: {spreadsheet.url}\")\n",
        "\n",
        "    # Place new spreadsheet into the folder\n",
        "    file_id = spreadsheet.id\n",
        "    drive_service.files().update(fileId=file_id,\n",
        "                                  addParents=folder_id,\n",
        "                                  removeParents='root', # Remove from root\n",
        "                                supportsAllDrives = True,\n",
        "                                  fields='id, parents').execute()\n",
        "    logging.info(f\"Moved spreadsheet to folder ID: {folder_id}\")\n",
        "\n",
        "  # Add or overwrite sheet within spreadsheet\n",
        "  try:\n",
        "    worksheet = spreadsheet.worksheet(sheet_name)\n",
        "    logging.info(f\"✅ Found worksheet: '{sheet_name}'. Clearing existing data...\")\n",
        "    worksheet.clear() # Clear all data\n",
        "  except WorksheetNotFound:\n",
        "    logging.info(f\"➕ Worksheet '{sheet_name}' not found. Creating new worksheet...\")\n",
        "    worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=len(df), cols=len(df.columns))\n",
        "\n",
        "  worksheet.update([df.columns.values.tolist()] + df.values.tolist())\n",
        "  logging.info(f\"DataFrame successfully uploaded to sheet '{sheet_name}' in spreadsheet '{spreadsheet.title}'.\")\n",
        "  return f\"{spreadsheet.url}#gid={worksheet.id}\"\n",
        "\n",
        "def gcs_csv_to_sheet(csv_path, gcs_folder):\n",
        "  '''This function reads a CSV file and uploads its content to a Google Sheet.\n",
        "  It requires authentication and uses the gspread library.\n",
        "\n",
        "  csv_path: The path to the CSV file.\n",
        "  gcs_folder: The name of the folder in gcs.\n",
        "  '''\n",
        "  df = pd.read_csv(csv_path)\n",
        "  df.replace(np.nan, None, inplace=True)\n",
        "  sheet_name = csv_path.split('/')[-1].removesuffix(\".csv\")\n",
        "  return df_to_sheet_in_folder(df, sheet_name, gcs_folder)\n",
        "\n",
        "\n",
        "def gcs_csv_to_df(csv_path, pydantic_model: BaseModel = NlQueryResponse) -> pd.DataFrame:\n",
        "  '''\n",
        "  This function reads a CSV file containing golden queries and converts it into\n",
        "  a pandas DataFrame. It uses converters to properly parse the nested data\n",
        "  structures within the 'dates', 'places', and 'variables' columns into their\n",
        "  respective Pydantic models.\n",
        "\n",
        "  csv_path: The path to the CSV file.\n",
        "  base_class: This value determines the appropriate converters to use for each column.\n",
        "  Returns: A pandas DataFrame.\n",
        "  '''\n",
        "\n",
        "  converters = {}\n",
        "  if issubclass(pydantic_model, NlQueryResponse):\n",
        "    converters= {\n",
        "        'dates': lambda dates: [DetectedDate.model_validate(x) for x in ast.literal_eval(dates)],\n",
        "        'places': lambda places: [DetectedPlace.model_validate(x) for x in ast.literal_eval(places)],\n",
        "        'variables': lambda vars: [VariableResponse.model_validate(x) for x in ast.literal_eval(vars)]\n",
        "        }\n",
        "\n",
        "  elif pydantic_model is NlQueryEvalScore:\n",
        "    converters= {\n",
        "        'golden_response': lambda x: NlGolden(**ast.literal_eval(x)),\n",
        "        'scraped_response': lambda x: NlApiScrape(**ast.literal_eval(x)),\n",
        "        'golden_type': GoldenType\n",
        "        }\n",
        "\n",
        "  return pd.read_csv(csv_path, converters=converters)\n",
        "\n",
        "\n",
        "def gcs_csv_to_pydantic_models(csv_path:str, pydantic_model: BaseModel) -> List[BaseModel]:\n",
        "  '''This function reads a CSV file and converts each row into a class object.\n",
        "\n",
        "  csv_path: The path to the CSV file.\n",
        "  pydantic_model: The Model Class the csv represents.\n",
        "  Returns: A list of NlQueryResponse objects.\n",
        "  '''\n",
        "  df = gcs_csv_to_df(csv_path, pydantic_model)\n",
        "  return df_to_pydantic_models(df, pydantic_model)\n",
        "\n",
        "def gcs_csv_to_variable_scrapes(csv_path: str, pydantic_model: NlQueryResponse = NlGolden) -> List[VariableResponse]:\n",
        "  '''This function reads a CSV file, extracts the 'variables' column, and combines all VariableResponse objects from that column into a single list.\n",
        "\n",
        "  csv_path: The path to the CSV file.\n",
        "  pydantic_model: The Model Class the csv represents.\n",
        "  Returns: A list of VariableResponse objects.\n",
        "  '''\n",
        "\n",
        "  df = pd.read_csv(csv_path, usecols=['variables'])\n",
        "  combined_variable_scrapes = [VariableResponse(**var) for vars in df['variables'] for var in ast.literal_eval(vars)]\n",
        "\n",
        "  return combined_variable_scrapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpGwp4qJTHMn"
      },
      "source": [
        "### Scrape API Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gPwxBVElkJ0"
      },
      "outputs": [],
      "source": [
        "# @title ### parse_dates\n",
        "from dataclasses import dataclass\n",
        "from abc import ABC\n",
        "\n",
        "# DateClassificationAttributes taken from\n",
        "# https://github.com/datacommonsorg/website/blob/b92b3f643e2b9116fcda9d0f631991b3c5a88fc7/server/lib/nl/detection/types.py#L338C1-L358C34\n",
        "@dataclass\n",
        "class Date:\n",
        "  \"\"\"Represents a range of two numeric quantities.\"\"\"\n",
        "  prep: str\n",
        "  year: int\n",
        "  month: Optional[int] = 0\n",
        "  year_span: Optional[int] = 0\n",
        "\n",
        "  def __str__(self):\n",
        "    return f'{self.year} - {self.month} | {self.year_span}'\n",
        "\n",
        "@dataclass\n",
        "class DateClassificationAttributes(ABC):\n",
        "  dates: List[Date]\n",
        "  is_single_date: bool\n",
        "  date_trigger_strings: List[str]\n",
        "\n",
        "# Taken from\n",
        "# https://github.com/datacommonsorg/website/blob/b92b3f643e2b9116fcda9d0f631991b3c5a88fc7/server/lib/nl/detection/date.py#L182C1-L201C30\n",
        "def get_date_range_strings(date: Date) -> (str, str):\n",
        "  _LAST_YEARS_PREP = 'last_years'\n",
        "  _START_DATE_PREPS = ['after', 'since', 'from', _LAST_YEARS_PREP]\n",
        "  _END_DATE_PREPS = ['before', 'by', 'until']\n",
        "  _MIN_MONTH = 1\n",
        "  _EXCLUSIVE_DATE_PREPS = ['before', 'after']\n",
        "  _MIN_DOUBLE_DIGIT_MONTH = 10\n",
        "\n",
        "\n",
        "  # Gets the year and month to use as the base date when calculating a date range\n",
        "  def _get_base_year_month(date: Date) -> (int, int):\n",
        "\n",
        "    base_year = date.year\n",
        "    base_month = date.month\n",
        "    # if date range excludes the specified date, need to do some calculations to\n",
        "    # get the base date.\n",
        "    if date.prep in _EXCLUSIVE_DATE_PREPS:\n",
        "      # if specified date is an end date, base date should be earlier than\n",
        "      # specified date\n",
        "      if date.prep in _END_DATE_PREPS:\n",
        "        # if date is monthly, use date that is one month before the specified date\n",
        "        if base_month >= _MIN_MONTH:\n",
        "          base_date = dt_date(base_year, base_month,\n",
        "                                    1) - relativedelta(months=1)\n",
        "          base_year = base_date.year\n",
        "          base_month = base_date.month\n",
        "        # otherwise, use date that is one year before the specified date\n",
        "        else:\n",
        "          base_year = base_year - 1\n",
        "      # if specified date is a start date, base date should be later than\n",
        "      # specified date\n",
        "      elif date.prep in _START_DATE_PREPS:\n",
        "        # if date is monthly, use date that is one month after the specified date\n",
        "        if base_month >= _MIN_MONTH:\n",
        "          base_date = dt_date(base_year, base_month,\n",
        "                                    1) + relativedelta(months=1)\n",
        "          base_year = base_date.year\n",
        "          base_month = base_date.month\n",
        "        # otherwise, use date that is one year after the specified date\n",
        "        else:\n",
        "          base_year = base_year + 1\n",
        "\n",
        "    return base_year, base_month\n",
        "\n",
        "\n",
        "  def _get_month_string(month: int) -> str:\n",
        "    month_string = ''\n",
        "    if month >= _MIN_DOUBLE_DIGIT_MONTH:\n",
        "      month_string = f'-{month}'\n",
        "    elif month >= _MIN_MONTH:\n",
        "      month_string = f'-0{month}'\n",
        "    return month_string\n",
        "\n",
        "  start_date = ''\n",
        "  end_date = ''\n",
        "  if not date or not date.year:\n",
        "    return start_date, end_date\n",
        "  base_year, base_month = _get_base_year_month(date)\n",
        "  year_string = str(base_year)\n",
        "  month_string = _get_month_string(base_month)\n",
        "  base_date = year_string + month_string\n",
        "  if date.prep in _START_DATE_PREPS:\n",
        "    start_date = base_date\n",
        "    if date.year_span > 0:\n",
        "      end_year = base_year + date.year_span\n",
        "      end_date = str(end_year) + month_string\n",
        "  elif date.prep in _END_DATE_PREPS:\n",
        "    end_date = base_date\n",
        "    if date.year_span > 0:\n",
        "      start_year = base_year - date.year_span\n",
        "      start_date = str(start_year) + month_string\n",
        "  return start_date, end_date\n",
        "\n",
        "\n",
        "def parse_dates(dbg_info):\n",
        "  date_classification = dbg_info.get('date_classification', '')\n",
        "\n",
        "  if not date_classification or date_classification == '<None>':\n",
        "    return []\n",
        "\n",
        "  attributes = eval(date_classification)\n",
        "\n",
        "  if not attributes.dates or len(attributes.dates) > 1 :\n",
        "    print('dates list length != 1')\n",
        "    return []\n",
        "\n",
        "  if attributes.is_single_date:\n",
        "    start_date = str(attributes.dates[0].year)\n",
        "    if attributes.dates[0].month:\n",
        "      start_date = f'{start_date}-{attributes.dates[0].month:02d}'\n",
        "    return [DetectedDate(base_date=start_date)]\n",
        "\n",
        "  detected_date = attributes.dates[0]\n",
        "  date_range = get_date_range_strings(detected_date)\n",
        "  if date_range == ('',''):\n",
        "    return empty_dates\n",
        "  start_date = date_range[0] if date_range[0] else today()\n",
        "  end_date = date_range[1] if date_range[1] else today()\n",
        "  return [DetectedDate(base_date=start_date, end_date=end_date)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUU4fDw50NQP"
      },
      "outputs": [],
      "source": [
        "# @title ### parse_places\n",
        "\n",
        "def parse_places(id, query, dbg_info):\n",
        "  # 1. Identify sub_place_type such as \"County\" or \"State\"\n",
        "  contained_in_classification = dbg_info.get('contained_in_classification', '<None>') # \"<None>\"\" matches the 'default' value returned by debug_info\n",
        "  sub_place_type = contained_in_classification.split('.')[-1] if contained_in_classification != '<None>' else None\n",
        "  if sub_place_type and 'DEFAULT_TYPE' in sub_place_type:\n",
        "    sub_place_type = None\n",
        "\n",
        "  # 2. Identify the detected place dcids\n",
        "  places = []\n",
        "\n",
        "  detection_logs =  dbg_info.get('query_detection_debug_logs', {})\n",
        "  # print(detection_logs)\n",
        "  # print(contained_in_classification)\n",
        "\n",
        "  try:\n",
        "    detected_places = detection_logs.get('place_resolution', {}).get('dc_resolved_places', {})\n",
        "    for detected_place in detected_places:\n",
        "      places.append(DetectedPlace(dcid=detected_place['dcid'], sub_place_type=sub_place_type))\n",
        "  except AttributeError as e:\n",
        "    logging.info(f'[places] no dc_resolved_places; {id} (\"{query}\")')\n",
        "    if sub_place_type:\n",
        "      logging.debug(f'[places] detected only sub_place_type; {id} (\"{query}\")')\n",
        "      places.append(DetectedPlace(dcid='', sub_place_type=sub_place_type))\n",
        "      return places\n",
        "\n",
        "    llm_response = dbg_info.get('llm_response', {})\n",
        "    if llm_response:\n",
        "      # TODO: verify if skipping llm_response when sub_place_type is present is correct\n",
        "      logging.info(f'[places] llm_response:\", )', dbg_info.get('llm_response', {}))\n",
        "\n",
        "      llm_sub_place = llm_response.get('SUB_PLACE_TYPE', '')\n",
        "      if 'DEFAULT_TYPE' in llm_sub_place:\n",
        "        llm_sub_place = None\n",
        "      if llm_sub_place:\n",
        "        places.append(DetectedPlace(dcid='', sub_place_type=llm_sub_place))\n",
        "    else:\n",
        "      logging.debug(f'[places] no place detected; {id} (\"{query}\")')\n",
        "\n",
        "  return places"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92ebmgyde76h"
      },
      "outputs": [],
      "source": [
        "# @title ### parse_variables\n",
        "\n",
        "def unfurl_group(topic_dcid, processed_topics):\n",
        "  dcids = []\n",
        "  processed_topic = processed_topics.get(topic_dcid, {})\n",
        "\n",
        "  for peer_group in processed_topic.get('peer_groups', []):\n",
        "    dcids.extend(peer_group[1]) # peer group structure is [peer_group_dcid, [stat_vars]]\n",
        "\n",
        "  for sub_topic in processed_topic.get('sub_topics', []):\n",
        "    dcids.extend(unfurl_group(sub_topic, processed_topics))\n",
        "\n",
        "  dcids.extend(processed_topic.get('svs', []))\n",
        "\n",
        "  return dcids\n",
        "\n",
        "def unfurl_dcids(dcids, processed_topics, id, query):\n",
        "  flat_vars = []\n",
        "  for dcid in dcids:\n",
        "    if dcid.startswith('dc/topic'):\n",
        "      if dcid not in processed_topics:\n",
        "        logging.info(f'[vars] topic not processed: {dcid}; {id} (\"{query}\")')\n",
        "        continue\n",
        "      flat_vars.extend(unfurl_group(dcid, processed_topics))\n",
        "    else:\n",
        "      flat_vars.append(dcid)\n",
        "\n",
        "  return flat_vars\n",
        "\n",
        "def parse_variables(id, query, dbg_info) -> list[VariableResponse]:\n",
        "\n",
        "  sv_matching = dbg_info.get('sv_matching', {})\n",
        "  detection_type = dbg_info.get('detection_type')\n",
        "  query_detection_logs = dbg_info.get('query_detection_debug_logs')\n",
        "  info_logs = dbg_info.get('counters', {}).get('INFO', {})\n",
        "\n",
        "  topics_processed = {}\n",
        "  for topic in info_logs.get('topics_processed', []):\n",
        "    topics_processed.update(topic)\n",
        "\n",
        "\n",
        "  single_sv_best_score = (sv_matching.get('CosineScore', [0]) + [0])[0]\n",
        "\n",
        "  multi_sv_candidate = None\n",
        "  if 'MultiSV' in sv_matching:\n",
        "    for candidate in sv_matching.get('MultiSV', {}).get('Candidates', []):\n",
        "      if candidate['DelimBased'] and len(candidate['Parts']) == 2:\n",
        "        # 0.05 matches the logic in\n",
        "        # https://github.com/datacommonsorg/website/blob/12f305f6525bd5d34d45d564503f827dcad2a9ee/shared/lib/constants.py#L458\n",
        "        if candidate['AggCosineScore'] + 0.05 >= single_sv_best_score:\n",
        "          variables = []\n",
        "          multi_sv_candidate = candidate\n",
        "          for part in multi_sv_candidate['Parts']:\n",
        "            variables.append(VariableResponse(search_label=part['QueryPart'], dcids=unfurl_dcids(part['SV'], topics_processed, id, query)))\n",
        "          return variables\n",
        "\n",
        "\n",
        "  ranking = None\n",
        "  if 'ranking_classification' in dbg_info:\n",
        "    ranking_classification = dbg_info.get('ranking_classification')\n",
        "    if 'HIGH' in ranking_classification:\n",
        "      ranking = Ranking.HIGH\n",
        "    elif 'LOW' in ranking_classification:\n",
        "      ranking = Ranking.LOW\n",
        "\n",
        "\n",
        "  if 'RICH' in dbg_info.get('superlative_classification', ''):\n",
        "    search_label = \"RICH\"\n",
        "    var_dcids = unfurl_dcids(info_logs.get('filtered_svs', [])[0], topics_processed, id, query)\n",
        "\n",
        "  elif 'POOR' in dbg_info.get('superlative_classification', ''):\n",
        "    search_label = \"POOR\"\n",
        "    var_dcids = unfurl_dcids(topics_processed.keys(), topics_processed, id, query)\n",
        "\n",
        "  elif detection_type == 'Hybrid - Heuristic Based' or detection_type=='Heuristic Based':\n",
        "    search_label=query_detection_logs.get('query_transformations', {}).get('sv_detection_query_stop_words_removal', '')\n",
        "    if not search_label:\n",
        "      logging.warning(f'[vars] no sv_detection_query_stop_words_removal, using empty str; {id} (\"{query}\") {query_detection_logs} {sv_matching}')\n",
        "    var_dcids = unfurl_dcids(info_logs.get('filtered_svs', [])[0], topics_processed, id, query)\n",
        "\n",
        "\n",
        "  elif detection_type == 'Hybrid - LLM Fallback' or detection_type=='LLM Based':\n",
        "    variable_strs = query_detection_logs['llm_response']['METRICS']\n",
        "    if len(variable_strs) > 1:\n",
        "      logging.warning('[vars] multiple llm detected statvars')\n",
        "\n",
        "    search_label=variable_strs[0]\n",
        "    var_dcids = unfurl_dcids(info_logs.get('filtered_svs', [])[0], topics_processed, id, query)\n",
        "\n",
        "  else:\n",
        "    logging.warning(f'[vars] different detection mode; {id} (\"{query}\")')\n",
        "\n",
        "\n",
        "  return [VariableResponse(search_label=search_label, dcids=var_dcids, rank=ranking)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title scrape query (main)\n",
        "\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "\n",
        "def scrape_query(id, query, detector_type='hybrid'):\n",
        "  response = None\n",
        "  query_response = None\n",
        "  scrape_date = today()\n",
        "\n",
        "  try:\n",
        "    response = requests.post(f'{host_website}/api/explore/detect-and-fulfill?q={query}&detector={detector_type}', json={}, timeout=None)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "      logging.warning(f'[api]  NL API request failed with status code {response.status_code}; {id} (\"{query}\")')\n",
        "      return NlApiScrape(id=id, query=query, dates=[], places=[], variables=[], api_response_status=ResponseStatus.ERROR, scrape_date=scrape_date)\n",
        "    query_response = response.json()\n",
        "\n",
        "  except json.JSONDecodeError as e:\n",
        "    raise json.JSONDecodeError(f'[api] NL API response is not valid JSON; {id} (\"{query}\"); {e}')\n",
        "\n",
        "  logging.debug(query_response)\n",
        "\n",
        "  dbg_info = query_response.get('debug', {})\n",
        "\n",
        "  if dbg_info.get('blocked', False):\n",
        "    logging.warning(f'[api] NL API blocked request; {id} (\"{query}\")')\n",
        "    return NlApiScrape(id=id, query=query, dates=[], places=[], variables=[], api_blocked=True, scrape_date=scrape_date)\n",
        "\n",
        "  dates = parse_dates(dbg_info)\n",
        "\n",
        "  places = parse_places(id, query, dbg_info)\n",
        "\n",
        "  variables = parse_variables(id, query, dbg_info)\n",
        "\n",
        "  return NlApiScrape(id=id, query=query, dates=dates, places=places, variables=variables, scrape_date=scrape_date)\n",
        "\n",
        "# -- Sanity Test --\n",
        "# logging.basicConfig(\n",
        "#     level=logging.INFO,\n",
        "#     format='%(levelname)s - %(message)s',\n",
        "#     force=True\n",
        "# )\n",
        "# scrape_query(0, 'What is the demographics of students in Sunnyvale')"
      ],
      "metadata": {
        "id": "DLxmIy5V9lld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgO82JDdKAyY"
      },
      "source": [
        "### Scoring Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haRnLniVD-C3"
      },
      "outputs": [],
      "source": [
        "!pip install python-dateutil --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUOxxvvPvgQf"
      },
      "outputs": [],
      "source": [
        "# @title ### score_date\n",
        "\n",
        "def calculate_date_scores(golden_dates: pd.Series, scrape_dates: pd.Series, date_of_scrape: pd.Series) -> pd.Series:\n",
        "  \"\"\"\n",
        "  Calculates date scores. TODO: elaborate\n",
        "  \"\"\"\n",
        "\n",
        "  def score_dates(golden_dates, scraped_dates, date_of_scrape):\n",
        "\n",
        "    def replace_placeholder(date_str):\n",
        "      if date_str == DATE_PLACEHOLDER:\n",
        "        return date_of_scrape\n",
        "\n",
        "      placeholder_pattern = r\"^\\$TODAY([+-])(\\d+)([YM])$\"\n",
        "      placeholders = re.fullmatch(placeholder_pattern, date_str)\n",
        "\n",
        "      if not placeholders:\n",
        "        logging.error(f'Unable to parse date placeholder {date_str}')\n",
        "        return date_str\n",
        "\n",
        "      sign, adjustment, unit = placeholders.groups()\n",
        "      adjustment = int(adjustment)\n",
        "\n",
        "      date_obj = datetime.strptime(date_of_scrape, DATE_OF_SCRAPE_FORMAT)\n",
        "\n",
        "      if unit == 'Y' and sign == '+':\n",
        "          return (date_obj + relativedelta(years=adjustment)).strftime('%Y')\n",
        "      elif unit == 'Y' and sign == '-':\n",
        "        return (date_obj - relativedelta(years=adjustment)).strftime('%Y')\n",
        "      elif unit == 'M' and sign == '+':\n",
        "        return (date_obj + relativedelta(months=adjustment)).strftime('%Y-%m')\n",
        "      elif unit == 'M' and sign == '-':\n",
        "        return (date_obj - relativedelta(months=adjustment)).strftime('%Y-%m')\n",
        "      else:\n",
        "        logging.error(f'Unable to parse date placeholder {date_str}')\n",
        "        return date_str\n",
        "\n",
        "    # If there wasn't a date in the query and the scrape did not hallucinate one,\n",
        "    # return \"empty\" so that this does not positively or negatively impact total\n",
        "    # scoring.\n",
        "    if not golden_dates and not scraped_dates:\n",
        "      return np.nan\n",
        "\n",
        "    # If either golden or scraped is present without the other, then automatic 0.\n",
        "    # (Either we detected dates when goldens say there's none to detect or we\n",
        "    # failed to detect dates when goldens say they are present in the query.)\n",
        "    if bool(golden_dates) ^ bool(scraped_dates):\n",
        "      return 0.0\n",
        "\n",
        "    # Populate any $TODAY based placeholders in the goldens with values\n",
        "    for golden in golden_dates:\n",
        "      if DATE_PLACEHOLDER in golden.base_date:\n",
        "        golden.base_date = replace_placeholder(golden.base_date)\n",
        "\n",
        "      if DATE_PLACEHOLDER in golden.end_date:\n",
        "        golden.end_date = replace_placeholder(golden.end_date)\n",
        "\n",
        "    individual_date_scores = []\n",
        "    for scrape in scraped_dates:\n",
        "      best_score = 0\n",
        "      for golden in golden_dates:\n",
        "\n",
        "        base_date_score = 1.0 if scrape.base_date == golden.base_date else 0.0\n",
        "        end_date_score = 1.0 if scrape.end_date == golden.end_date else 0.0\n",
        "        score = statistics.mean([base_date_score, end_date_score])\n",
        "\n",
        "        if score > best_score:\n",
        "          best_score = score\n",
        "\n",
        "      individual_date_scores.append(best_score)\n",
        "\n",
        "    individual_date_scores.extend([0.0] * abs(len(golden_dates) - len(scraped_dates)))\n",
        "\n",
        "    return statistics.mean(individual_date_scores)\n",
        "\n",
        "  return [score_dates(g, s, d) for g, s, d in zip(golden_dates, scrape_dates, date_of_scrape)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKE6bB5hy57s"
      },
      "outputs": [],
      "source": [
        "# @title calculate_fbeta (used by places + variables)\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "def calculate_score(y_true_sets, y_pred_sets, beta=1.0):\n",
        "  def per_sample_fbeta_score(y_true, y_pred, beta=1.0):\n",
        "    \"\"\"\n",
        "    Calculates the F-beta score for each sample in a multilabel setting using\n",
        "    fast, vectorized NumPy operations.\n",
        "\n",
        "    Args:\n",
        "        y_true (np.ndarray): A (n_samples, n_classes) binary matrix of true labels.\n",
        "        y_pred (np.ndarray): A (n_samples, n_classes) binary matrix of predicted labels.\n",
        "        beta (float): The beta value for the F-beta score.\n",
        "\n",
        "    Returns:\n",
        "        A (n_samples,) array of F-beta scores for each sample.\n",
        "    \"\"\"\n",
        "    y_true = y_true.astype(bool)\n",
        "    y_pred = y_pred.astype(bool)\n",
        "\n",
        "    tp = (y_true & y_pred).sum(axis=1)\n",
        "    fp = (y_pred & ~y_true).sum(axis=1)\n",
        "    fn = (y_true & ~y_pred).sum(axis=1)\n",
        "\n",
        "    beta_sq = beta**2\n",
        "\n",
        "    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp, dtype=float), where=(tp + fp) > 0)\n",
        "    recall = np.divide(tp, tp + fn, out=np.zeros_like(tp, dtype=float), where=(tp + fn) > 0)\n",
        "\n",
        "    fbeta = np.divide(\n",
        "        (1 + beta_sq) * precision * recall,\n",
        "        (beta_sq * precision) + recall,\n",
        "        out=np.zeros_like(tp, dtype=float),\n",
        "        where=((beta_sq * precision) + recall) > 0\n",
        "    )\n",
        "\n",
        "    fbeta[(tp + fp + fn) == 0] = 1.0\n",
        "\n",
        "    return np.round(fbeta, decimals=3), np.round(precision, decimals=3), np.round(recall, decimals=3)\n",
        "\n",
        "  mlb = MultiLabelBinarizer()\n",
        "  all_labels = y_true_sets.tolist() + y_pred_sets.tolist()\n",
        "\n",
        "  mlb.fit(all_labels)\n",
        "\n",
        "  y_true_matrix = mlb.transform(y_true_sets)\n",
        "  y_pred_matrix = mlb.transform(y_pred_sets)\n",
        "\n",
        "  return per_sample_fbeta_score(y_true_matrix, y_pred_matrix, beta=beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03steDlcvkWj"
      },
      "outputs": [],
      "source": [
        "# @title ### score_place\n",
        "\n",
        "def calculate_place_scores(golden_places_col: pd.Series, scrape_places_col: pd.Series) -> pd.Series:\n",
        "\n",
        "  def get_place_dcid_set(places: list[DetectedPlace]) -> set[str]:\n",
        "    return {place.dcid for place in places}\n",
        "\n",
        "  def get_sub_type_set(places: list[DetectedPlace]) -> set[str]:\n",
        "    labels = set()\n",
        "    for place in places:\n",
        "      if place.sub_place_type:\n",
        "        labels.add(place.sub_place_type)\n",
        "    return labels\n",
        "\n",
        "  def get_place_labels_set(places: list[DetectedPlace]) -> set[str]:\n",
        "    labels = set()\n",
        "    for place in places:\n",
        "      sub_place_type = place.sub_place_type if place.sub_place_type else ''\n",
        "      labels.add(f\"{place.dcid}:{sub_place_type}\")\n",
        "    return labels\n",
        "\n",
        "\n",
        "  # phase 1 - place_dcid scoring; this is to give \"partial credit\" for when a\n",
        "  # parent place is properly detected even when the child place is not\n",
        "  y_true_place_dcids = golden_places_col.apply(get_place_dcid_set)\n",
        "  y_pred_place_dcids = scrape_places_col.apply(get_place_dcid_set)\n",
        "\n",
        "  place_dcids_scores, _, _ = calculate_score(y_true_place_dcids, y_pred_place_dcids, beta=0.8)\n",
        "\n",
        "  # phase 2 - child place type accuracy\n",
        "  y_true_sub_place_types = golden_places_col.apply(get_sub_type_set)\n",
        "  y_pred_sub_place_types = scrape_places_col.apply(get_sub_type_set)\n",
        "\n",
        "  sub_place_type_scores,_,_ = calculate_score(y_true_sub_place_types, y_pred_sub_place_types)\n",
        "\n",
        "  # phase 3 - full place_dcid +/- child type pairs\n",
        "  y_true_full_place = golden_places_col.apply(get_place_labels_set)\n",
        "  y_pred_full_place =  scrape_places_col.apply(get_place_labels_set)\n",
        "\n",
        "  full_place_scores,_,_ = calculate_score(y_true_full_place, y_pred_full_place, beta=0.5)\n",
        "\n",
        "  places_weight = 0.4\n",
        "  child_types_weight = 0.2\n",
        "  full_weight = 0.4\n",
        "\n",
        "  combined_score = pd.Series((places_weight * place_dcids_scores) +\n",
        "                    (child_types_weight * sub_place_type_scores) +\n",
        "                    (full_weight * full_place_scores))\n",
        "\n",
        "  # For cases where there are no places in the goldens, we shouldn't produce a score\n",
        "  # instead, populate with NaN.\n",
        "  do_not_score = (golden_places_col.str.len() == 0) & (scrape_places_col.str.len() == 0)\n",
        "  combined_score.loc[do_not_score] = np.nan\n",
        "\n",
        "  return combined_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GqhAp7f7wF5"
      },
      "outputs": [],
      "source": [
        "# @title ### score_variables\n",
        "\n",
        "def calculate_variable_scores(golden_vars_col: pd.Series, scrape_vars_col: pd.Series) -> pd.Series:\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def get_var_dcid_set(vars: list[VariableResponse]) -> set[str]:\n",
        "    return {dcid for var in vars for dcid in var.dcids}\n",
        "\n",
        "  # phase 1 - variable dcid scoring\n",
        "\n",
        "  y_true_var_dcids = golden_vars_col.apply(get_var_dcid_set)\n",
        "  y_pred_var_dcids = scrape_vars_col.apply(get_var_dcid_set)\n",
        "\n",
        "  # Use high beta score to favor recall over precision - care more about finding\n",
        "  # right statvars than excluding non-required ones.\n",
        "  var_dcid_fbeta, precision, recall = calculate_score(y_true_var_dcids, y_pred_var_dcids, beta=2)\n",
        "\n",
        "  combined_score = pd.Series(var_dcid_fbeta)\n",
        "  precision = pd.Series(precision)\n",
        "  recall = pd.Series(recall)\n",
        "\n",
        "  # TODO: once all goldens have variables, uncomment the following line and delete the next\n",
        "  # do_not_score = (golden_vars_col.str.len() == 0) & (scrape_vars_col.str.len() == 0)\n",
        "  do_not_score = (golden_vars_col.str.len() == 0)\n",
        "  combined_score.loc[do_not_score] = np.nan\n",
        "  precision.loc[do_not_score] = np.nan\n",
        "  recall.loc[do_not_score] = np.nan\n",
        "\n",
        "  return combined_score, precision, recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv08t8FjJg-p"
      },
      "outputs": [],
      "source": [
        "# @title Score Epoch\n",
        "\n",
        "import statistics\n",
        "import re\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from pydantic import model_validator\n",
        "from typing import Optional, Any\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "DATE_OF_SCRAPE_FORMAT = '%Y-%m-%d'\n",
        "DATE_PLACEHOLDER = '$TODAY'\n",
        "\n",
        "\n",
        "def calculate_total_scores(date_scores: pd.Series, place_scores: pd.Series, variable_scores: pd.Series) -> pd.Series:\n",
        "\n",
        "    # 1. Define the weights for each component\n",
        "    weights = {\n",
        "        'date': 0.2,\n",
        "        'place': 0.4,\n",
        "        'variable': 0.4\n",
        "    }\n",
        "\n",
        "    # 2. Create a DataFrame from the input Series for easier operations\n",
        "    tmp_df = pd.DataFrame({\n",
        "        'date': date_scores,\n",
        "        'place': place_scores,\n",
        "        'variable': variable_scores\n",
        "    })\n",
        "\n",
        "    # 3. Calculate the Numerator (the weighted sum of scores)\n",
        "    # We replace NaNs with 0 before multiplying by the weight. This ensures that\n",
        "    # missing components contribute nothing to the sum, which is correct.\n",
        "    numerator = (tmp_df['date'].fillna(0) * weights['date']) + \\\n",
        "                (tmp_df['place'].fillna(0) * weights['place']) + \\\n",
        "                (tmp_df['variable'].fillna(0) * weights['variable'])\n",
        "\n",
        "    # 4. Calculate the Dynamic Denominator (the sum of weights for non-NaN scores)\n",
        "    # First, create a boolean DataFrame (True where scores exist)\n",
        "    not_na_df = tmp_df.notna()\n",
        "\n",
        "    # Multiply the boolean DataFrame by the weights. True becomes 1, False becomes 0.\n",
        "    # This gives us the weight of each component IF it had a score, otherwise 0.\n",
        "    applicable_weights_df = not_na_df * pd.Series(weights)\n",
        "\n",
        "    # Sum these weights horizontally (axis=1) to get the total weight for each row\n",
        "    denominator = applicable_weights_df.sum(axis=1)\n",
        "\n",
        "    # 5. Calculate the Final Score\n",
        "    # We use np.divide for safe division, which correctly produces NaN\n",
        "    # if the denominator is 0 (i.e., all scores for a row were NaN).\n",
        "    final_scores = np.divide(numerator, denominator, out=np.full_like(denominator, np.nan), where=denominator!=0)\n",
        "\n",
        "    return pd.Series(final_scores)\n",
        "\n",
        "\n",
        "def compute_scores(goldens_df, scrapes_df):\n",
        "  goldens_df = goldens_df.rename(columns=lambda c: f\"{c}_golden\" if c != 'id' else c)\n",
        "  scrapes_df = scrapes_df.rename(columns=lambda c: f\"{c}_scraped\" if c != 'id' else c)\n",
        "  merged_df = pd.merge(goldens_df, scrapes_df, on='id')\n",
        "\n",
        "  # Drop rows if query is not the same for golden and scraped response\n",
        "  mismatched_query_mask = merged_df['query_golden'] != merged_df['query_scraped']\n",
        "  if mismatched_query_mask.any():\n",
        "    mismatched_ids = merged_df.loc[mismatched_query_mask, 'id'].tolist()\n",
        "    logging.error(f'id collision: same id, different query for {mismatched_ids}; dropping from eval')\n",
        "    merged_df = merged_df[~mismatched_query_mask]\n",
        "\n",
        "  score_df = pd.DataFrame()\n",
        "  score_df['id'] = merged_df['id']\n",
        "  score_df['query'] = merged_df['query_golden']\n",
        "  score_df['golden_type'] = merged_df['golden_type_golden']\n",
        "\n",
        "  # Calculate all scores using vectorized operations; this is more efficient than iterating rows.\n",
        "  score_df['date_score'] = calculate_date_scores(merged_df['dates_golden'], merged_df['dates_scraped'], merged_df['scrape_date_scraped'])\n",
        "  score_df['place_score'] = calculate_place_scores(merged_df['places_golden'], merged_df['places_scraped'])\n",
        "  score_df['variable_score'], score_df['variable_precision'], score_df['variable_recall'] = calculate_variable_scores(merged_df['variables_golden'], merged_df['variables_scraped'])\n",
        "\n",
        "  score_df['total_score'] = calculate_total_scores(score_df['date_score'], score_df['place_score'], score_df['variable_score'])\n",
        "\n",
        "  golden_cols = ['id'] + [col for col in merged_df.columns if col.endswith('_golden')]\n",
        "  merged_goldens_df = merged_df[golden_cols].rename(\n",
        "    columns=lambda c: c.removesuffix('_golden')\n",
        ")\n",
        "  score_df['golden_response'] = df_to_pydantic_models(merged_goldens_df, NlGolden)\n",
        "\n",
        "  scraped_cols = ['id'] + [col for col in merged_df.columns if col.endswith('_scraped')]\n",
        "  merged_srapes_df = merged_df[scraped_cols].rename(\n",
        "    columns=lambda c: c.removesuffix('_scraped')\n",
        ")\n",
        "  score_df['scraped_response'] = df_to_pydantic_models(merged_srapes_df, NlApiScrape)\n",
        "\n",
        "  return score_df\n",
        "\n",
        "def run_epoch_score(golden_epoch, scrape_epoch, score_epoch, description, change_log):\n",
        "  golden_path = goldens_path(golden_epoch)\n",
        "  scrape_path = scrapes_path(scrape_epoch)\n",
        "  score_output_path = scores_path(scrape_epoch, score_epoch)\n",
        "  print(score_output_path)\n",
        "\n",
        "  scrape_df = gcs_csv_to_df(scrape_path, NlApiScrape)\n",
        "  golden_df = gcs_csv_to_df(golden_path, NlGolden)\n",
        "\n",
        "  score_df = compute_scores(golden_df, scrape_df)\n",
        "\n",
        "\n",
        "\n",
        "  df_to_gcs_csv(score_output_path, score_df, NlQueryEvalScore)\n",
        "\n",
        "  summary = pd.DataFrame()\n",
        "  cols = ['total_score', 'date_score', 'place_score', 'variable_score', 'variable_precision', 'variable_recall']\n",
        "  summary['overall'] = score_df[cols].mean().round(3)\n",
        "  summary['stable']  = score_df[score_df['golden_type'] == 'STABLE'][cols].mean().round(3)\n",
        "  summary['aspirational']  = score_df[score_df['golden_type'] == 'ASPIRATIONAL'][cols].mean().round(3)\n",
        "\n",
        "  summary.to_csv('local_summary.csv')\n",
        "\n",
        "  gcs_summary_path = summary_path(scrape_epoch, score_epoch)\n",
        "\n",
        "  metadata = EvalMetadata(golden_epoch = golden_epoch, scrape_epoch=scrape_epoch, score_epoch=score_epoch, description=description, change_log=change_log)\n",
        "\n",
        "  # 6. Append metadata to the CSV\n",
        "  with open('local_summary.csv', 'a') as f: # Use 'a' for append mode\n",
        "      f.write(\",\\n\") # Add a newline for separation\n",
        "      f.write(\",\\n\") # Add a newline for separation\n",
        "      f.write(\"# --- METADATA ---\\n\") # Optional: a clear separator line\n",
        "      for field_name, value in metadata.model_dump().items(): # Use model_dump() for Pydantic v2+\n",
        "          if value is not None: # Only write fields that have a value\n",
        "              # Format as key=value or key,value. Using comma for CSV compatibility\n",
        "              f.write(f\"{field_name},{value}\\n\")\n",
        "\n",
        "  !gsutil cp local_summary.csv {gcs_summary_path} > /dev/null 2>&1\n",
        "\n",
        "  return score_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiPSxOGBxby1"
      },
      "source": [
        "## Scrape and/or Score an epoch\n",
        "\n",
        "### General Inputs\n",
        "This table describes the main input required to define the queryset for the evaluation.\n",
        "\n",
        "* Valid Golden File Names: [gcs link](https://pantheon.corp.google.com/storage/browser/datcom-nl-evals/goldens?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22,%22s%22:%5B(%22i%22:%22displayName%22,%22s%22:%221%22)%5D))&authuser=0&e=13803378&inv=1&invt=Ab3YaQ&mods=-monitoring_api_staging&prefix=&forceOnObjectsSortingFiltering=true)\n",
        "* Previous scrapes: [gcs link](https://pantheon.corp.google.com/storage/browser/datcom-nl-evals/evals?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22,%22s%22:%5B(%22i%22:%22displayName%22,%22s%22:%221%22)%5D))&authuser=0&e=13803378&inv=1&invt=Ab3YaQ&mods=-monitoring_api_staging)\n",
        "\n",
        "| Parameter | Type | Default Value | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `golden_epoch` | `string` | `--` | Required. The file name of the golden queryset (without extension). This defines the queries to be scraped and serves as the expected values for scoring. |\n",
        "| `scrape_epoch` | `string` | `--` | Required for scoring-only runs, optional when scraping. The unique identifier for a scrape run. If left empty, one is generated from the current date and time. **Caution:** Re-using an existing name will overwrite the previous scrape. |\n",
        "\n",
        "---\n",
        "\n",
        "### Scrape Related Inputs\n",
        "These inputs control the scraping process, which queries the API and saves the responses.\n",
        "\n",
        "| Parameter | Type | Default Value | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `run_scrape` | `boolean` | `False` | Set to `True` to query the detect-and-fulfill endpoint and save the responses as `NlApiScrape` objects. |\n",
        "| `detector_type` | `string` | `--` | Specifies the detector to use. Options include `\"hybrid\"`, `\"heuristic\"`, and `\"llm\"`. This is not used if `run_scrape` is `False`. |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Scoring Related Inputs\n",
        "These inputs control the scoring process, which compares the scraped results against the golden set and generates reports.\n",
        "\n",
        "| Parameter | Type | Default Value | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `run_scoring` | `boolean` | `False` | Set to `True` to score the scraped results against the golden set and generate a score report and summary. |\n",
        "| `eval_description` | `string` | `\"\"` | A brief description of the evaluation run. |\n",
        "| `eval_change_log` | `string` | `\"\"` | A log of any changes relevant to this evaluation run. |\n",
        "| `score_epoch` | `string` | `\"\"` | An optional, unique identifier for the scoring run. If left empty, one will be generated. **Caution:** Re-using an existing name will overwrite previous score reports. |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_Zr4t2OTJkV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown\n",
        "# @markdown ### Inputs:\n",
        "golden_epoch = \"2025-06-23\" # @param {\"type\":\"string\"}\n",
        "scrape_epoch = \"\" # @param {\"type\":\"string\", \"placeholder\":\"Optional for when running scrape, required for a score-only run. Created based on current date+time if empty\"}\n",
        "\n",
        "\n",
        "# @markdown #### Scrape Related Inputs\n",
        "run_scrape = True # @param {\"type\":\"boolean\"}\n",
        "detector_type = \"hybrid\" # @param [\"hybrid\",\"heuristic\",\"llm\"]\n",
        "\n",
        "# @markdown #### Scoring Related Inputs\n",
        "run_scoring = True # @param {\"type\":\"boolean\"}\n",
        "eval_description = \"\" # @param {\"type\":\"string\"}\n",
        "eval_change_log = \"\" # @param {\"type\":\"string\"}\n",
        "score_epoch = \"\" # @param {\"type\":\"string\", \"placeholder\":\"(optional, this will be generated based on current date+time if empty)\"}\n",
        "\n",
        "\n",
        "import concurrent.futures\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.WARNING,\n",
        "    format='%(levelname)s - %(message)s',\n",
        "    force=True\n",
        ")\n",
        "\n",
        "def scrape_queryset(queryset):\n",
        "  results_threading = []\n",
        "\n",
        "  with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "\n",
        "      future_to_query = {executor.submit(scrape_query, id, query, detector_type): (id, query) for id, query in queryset.items()}\n",
        "\n",
        "      for future in concurrent.futures.as_completed(future_to_query):\n",
        "          id, query = future_to_query[future]\n",
        "          try:\n",
        "              result = future.result()\n",
        "              results_threading.append(result)\n",
        "          except Exception as exc:\n",
        "              print(f'{id} (\"{query}\"): generated an exception: {exc}')\n",
        "  return results_threading\n",
        "\n",
        "#\n",
        "# ========================== MAIN ==========================\n",
        "#\n",
        "\n",
        "if not run_scrape and not run_scoring:\n",
        "  print('Nothing to do! Select at least one operation to perform')\n",
        "\n",
        "if run_scrape:\n",
        "\n",
        "  goldens_file_path = goldens_path(golden_epoch)\n",
        "  goldens_df = gcs_csv_to_df(goldens_file_path)\n",
        "  queryset = dict(zip(goldens_df['id'].astype(int), goldens_df['query'].astype(str)))\n",
        "\n",
        "  if not scrape_epoch:\n",
        "    scrape_epoch = now()\n",
        "\n",
        "  print(f'scrape_epoch: {scrape_epoch}')\n",
        "\n",
        "  full_scrape = scrape_queryset(queryset)\n",
        "  print(f'num queries scraped: {len(full_scrape)}')\n",
        "\n",
        "\n",
        "  scrapes_gcs = scrapes_path(scrape_epoch)\n",
        "  print(f'scrape output path: {scrapes_gcs}')\n",
        "\n",
        "  # Write to GCS and Sheets\n",
        "  models_to_gcs_csv(scrapes_gcs, full_scrape)\n",
        "  gcs_csv_to_sheet(scrapes_gcs, scrape_epoch)\n",
        "\n",
        "if run_scoring:\n",
        "  if not score_epoch:\n",
        "    score_epoch = now()\n",
        "  print(f'score output path: {scores_path(scrape_epoch, score_epoch)}')\n",
        "\n",
        "  # Conduct Eval\n",
        "  score_df = run_epoch_score(golden_epoch, scrape_epoch, score_epoch, eval_description, eval_change_log)\n",
        "\n",
        "  # Transfer scores from GCS to Sheet\n",
        "  gcs_csv_to_sheet(scores_path(scrape_epoch, score_epoch), scrape_epoch)\n",
        "  summary_url = gcs_csv_to_sheet(summary_path(scrape_epoch, score_epoch), scrape_epoch)\n",
        "\n",
        "  print('------------------------------------------------------------------')\n",
        "  #print('>> View results at', summary_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Misc"
      ],
      "metadata": {
        "id": "0zR7EPFpX4QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy goldens from GCS to sheet!\n",
        "gcs_csv_to_sheet(goldens_path('2025-06-23'), 'goldens')"
      ],
      "metadata": {
        "id": "x7Gtk4QmLt-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IrmNK5yFZbO"
      },
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s66Mq36Elur3"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBB_tDIgllTz"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "\n",
        "class TestCalculateDateScores(unittest.TestCase):\n",
        "\n",
        "  def setUp(self):\n",
        "      \"\"\"Set up a mock date object structure for use in tests.\"\"\"\n",
        "      # Using SimpleNamespace is a quick way to create mock objects with attributes\n",
        "      self.Date = lambda base, end: DetectedDate(base_date=base, end_date=end)\n",
        "      self.date_of_scrape = '2025-06-11'\n",
        "\n",
        "  def test_no_golden_no_scraped(self):\n",
        "      \"\"\"Test score when both golden and scraped are empty; should be NaN.\"\"\"\n",
        "      golden_series = pd.Series([[]])\n",
        "      scraped_series = pd.Series([[]])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertTrue(np.isnan(result[0]))\n",
        "\n",
        "  def test_no_golden_with_scraped(self):\n",
        "      \"\"\"Test score when scrape hallucinates a date; should be 0.0.\"\"\"\n",
        "      golden_series = pd.Series([[]])\n",
        "      scraped_dates = [self.Date('2025-01-01', '2025-01-02')]\n",
        "      scraped_series = pd.Series([scraped_dates])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertEqual(result[0], 0.0)\n",
        "\n",
        "  def test_perfect_match(self):\n",
        "      \"\"\"Test score for a perfect match; should be 1.0.\"\"\"\n",
        "      dates = [self.Date('2025-01-01', '2025-01-02')]\n",
        "      golden_series = pd.Series([dates])\n",
        "      scraped_series = pd.Series([dates])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertEqual(result[0], 1.0)\n",
        "\n",
        "  def test_partial_match_base_date_only(self):\n",
        "      \"\"\"Test score when only the base date matches; should be 0.5.\"\"\"\n",
        "      golden_dates = [self.Date('2025-01-01', '2025-01-02')]\n",
        "      scraped_dates = [self.Date('2025-01-01', '2025-01-03')] # Different end_date\n",
        "      golden_series = pd.Series([golden_dates])\n",
        "      scraped_series = pd.Series([scraped_dates])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertEqual(result[0], 0.5)\n",
        "\n",
        "  def test_no_match(self):\n",
        "      \"\"\"Test score when dates do not match at all; should be 0.0.\"\"\"\n",
        "      golden_dates = [self.Date('2025-01-01', '2025-01-02')]\n",
        "      scraped_dates = [self.Date('2026-01-01', '2026-01-02')]\n",
        "      golden_series = pd.Series([golden_dates])\n",
        "      scraped_series = pd.Series([scraped_dates])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertEqual(result[0], 0.0)\n",
        "\n",
        "  def test_multiple_golden_finds_best_match(self):\n",
        "      \"\"\"Test that a single scrape finds its best score among multiple goldens.\"\"\"\n",
        "      golden_dates = [\n",
        "          self.Date('2025-01-01', '2025-01-02'), # No match\n",
        "          self.Date('2025-02-01', '2025-02-02')  # Perfect match\n",
        "      ]\n",
        "      scraped_dates = [self.Date('2025-02-01', '2025-02-02')]\n",
        "      golden_series = pd.Series([golden_dates])\n",
        "      scraped_series = pd.Series([scraped_dates])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertEqual(result[0], 1.0) # Score is 1.0 because it found the best match\n",
        "\n",
        "  def test_multiple_scraped_averages_scores(self):\n",
        "      \"\"\"Test that scores from multiple scraped dates are averaged.\"\"\"\n",
        "      golden_dates = [self.Date('2025-01-01', '2025-01-02')]\n",
        "      scraped_dates = [\n",
        "          self.Date('2025-01-01', '2025-01-02'), # Perfect match (score = 1.0)\n",
        "          self.Date('2026-01-01', '2026-01-02')  # No match (score = 0.0)\n",
        "      ]\n",
        "      golden_series = pd.Series([golden_dates])\n",
        "      scraped_series = pd.Series([scraped_dates])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertEqual(result[0], 0.5) # Average of 1.0 and 0.0\n",
        "\n",
        "  def test_golden_with_no_scraped(self):\n",
        "      \"\"\"Test score when scraper finds no dates but should have; should be 0.0.\"\"\"\n",
        "      golden_dates = [self.Date('2025-01-01', '2025-01-02')]\n",
        "      golden_series = pd.Series([golden_dates])\n",
        "      scraped_series = pd.Series([[]])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertEqual(result[0], 0.0)\n",
        "\n",
        "  def test_placeholder_today(self):\n",
        "      \"\"\"Test replacement of the $TODAY placeholder.\"\"\"\n",
        "      golden_dates = [self.Date('$TODAY', '$TODAY')]\n",
        "      scraped_dates = [self.Date('2025-06-11', '2025-06-11')]\n",
        "      golden_series = pd.Series([golden_dates])\n",
        "      scraped_series = pd.Series([scraped_dates])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertEqual(result[0], 1.0)\n",
        "\n",
        "  def test_placeholder_relative_add_year(self):\n",
        "      \"\"\"Test replacement of a relative date placeholder ($TODAY+1Y).\"\"\"\n",
        "      golden_dates = [self.Date('$TODAY+1Y', '$TODAY-1Y')]\n",
        "      scraped_dates = [self.Date('2026', '2024')]\n",
        "      golden_series = pd.Series([golden_dates])\n",
        "      scraped_series = pd.Series([scraped_dates])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertEqual(result[0], 1.0)\n",
        "\n",
        "  def test_placeholder_relative_subtract_month(self):\n",
        "      \"\"\"Test replacement of a relative date placeholder ($TODAY-2M).\"\"\"\n",
        "      golden_dates = [self.Date('$TODAY+2M', '$TODAY-2M')]\n",
        "      scraped_dates = [self.Date('2025-08', '2025-04')]\n",
        "      golden_series = pd.Series([golden_dates])\n",
        "      scraped_series = pd.Series([scraped_dates])\n",
        "      scrape_date_series = pd.Series([self.date_of_scrape])\n",
        "\n",
        "      result = calculate_date_scores(golden_series, scraped_series, scrape_date_series)\n",
        "      self.assertEqual(result[0], 1.0)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q64HyK8RetRz"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "\n",
        "class TestPlaceScoreCalculation(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Set up weights and the lambda function to create DetectedPlace objects.\"\"\"\n",
        "        self.parent_weight = 0.4\n",
        "        self.child_weight = 0.2\n",
        "        self.structure_weight = 0.4\n",
        "        # This lambda function makes creating test objects clean and easy\n",
        "        self.Place = lambda dcid='', sub_place_type=None: DetectedPlace(dcid=dcid, sub_place_type=sub_place_type)\n",
        "\n",
        "    def test_perfect_match(self):\n",
        "        \"\"\"Score should be 1.0 for a perfect match.\"\"\"\n",
        "        golden = pd.Series([[self.Place(dcid='geo_ca', sub_place_type='STATE')]])\n",
        "        scraped = pd.Series([[self.Place(dcid='geo_ca', sub_place_type='STATE')]])\n",
        "        result = calculate_place_scores(golden, scraped)\n",
        "        self.assertAlmostEqual(result[0], 1.0)\n",
        "\n",
        "    def test_partial_credit_parent_only(self):\n",
        "        \"\"\"Score should be parent_weight if only the parent matches.\"\"\"\n",
        "        golden = pd.Series([[self.Place(dcid='geo_ca', sub_place_type='STATE')]])\n",
        "        scraped = pd.Series([[self.Place(dcid='geo_ca')]]) # Missing sub_place_type\n",
        "        result = calculate_place_scores(golden, scraped)\n",
        "        self.assertAlmostEqual(result[0], self.parent_weight)\n",
        "\n",
        "    def test_partial_credit_child_only(self):\n",
        "        \"\"\"Score should be child_weight if only the child matches.\"\"\"\n",
        "        golden = pd.Series([[self.Place(dcid='geo_ca', sub_place_type='STATE')]])\n",
        "        scraped = pd.Series([[self.Place(sub_place_type='STATE')]]) # Missing dcid\n",
        "        result = calculate_place_scores(golden, scraped)\n",
        "        self.assertAlmostEqual(result[0], self.child_weight)\n",
        "\n",
        "    def test_partial_credit_wrong_parent_correct_child(self):\n",
        "        \"\"\"Score should be child_weight if child is right but parent is wrong.\"\"\"\n",
        "        golden = pd.Series([[self.Place(dcid='geo_ca', sub_place_type='STATE')]])\n",
        "        scraped = pd.Series([[self.Place(dcid='geo_us', sub_place_type='STATE')]])\n",
        "        result = calculate_place_scores(golden, scraped)\n",
        "        self.assertAlmostEqual(result[0], self.child_weight)\n",
        "\n",
        "    def test_total_miss(self):\n",
        "        \"\"\"Score should be 0.0 for a complete mismatch.\"\"\"\n",
        "        golden = pd.Series([[self.Place(dcid='geo_ca', sub_place_type='STATE')]])\n",
        "        scraped = pd.Series([[self.Place(dcid='geo_us', sub_place_type='COUNTY')]])\n",
        "        result = calculate_place_scores(golden, scraped)\n",
        "        self.assertAlmostEqual(result[0], 0.0)\n",
        "\n",
        "    def test_not_applicable_case(self):\n",
        "        \"\"\"Score should be NaN if both golden and scraped are empty.\"\"\"\n",
        "        golden = pd.Series([[]])\n",
        "        scraped = pd.Series([[]])\n",
        "        result = calculate_place_scores(golden, scraped)\n",
        "        self.assertTrue(np.isnan(result[0]))\n",
        "\n",
        "    def test_golden_expected_scrape_empty(self):\n",
        "        \"\"\"Score should be 0.0 if scrape misses an expected place.\"\"\"\n",
        "        golden = pd.Series([[self.Place(dcid='geo_ca', sub_place_type='STATE')]])\n",
        "        scraped = pd.Series([[]])\n",
        "        result = calculate_place_scores(golden, scraped)\n",
        "        self.assertAlmostEqual(result[0], 0.0)\n",
        "\n",
        "    def test_scrape_hallucinated_golden_empty(self):\n",
        "        \"\"\"Score should be 0.0 if scrape finds a place that wasn't expected.\"\"\"\n",
        "        golden = pd.Series([[]])\n",
        "        scraped = pd.Series([[self.Place(dcid='geo_ca', sub_place_type='STATE')]])\n",
        "        result = calculate_place_scores(golden, scraped)\n",
        "        self.assertAlmostEqual(result[0], 0.0)\n",
        "\n",
        "    def test_multiple_labels_and_rows(self):\n",
        "        \"\"\"Test with a multi-row DataFrame and multiple labels per row.\"\"\"\n",
        "        golden = pd.Series([\n",
        "            [self.Place('geo_ca', 'STATE'), self.Place('geo_us', 'COUNTRY')], # Row 0\n",
        "            [self.Place('geo_de')]                                           # Row 1\n",
        "        ])\n",
        "        scraped = pd.Series([\n",
        "            [self.Place('geo_ca', 'STATE'), self.Place('geo_us')], # Row 0: Partial match\n",
        "            [self.Place('geo_de', 'STATE')]                      # Row 1: Partial match\n",
        "        ])\n",
        "\n",
        "        result = calculate_place_scores(golden, scraped)\n",
        "\n",
        "        # Expected Score for Row 0\n",
        "        parent_score_0 = 1.0; child_score_0 = 0.666666; structure_score_0 = 0.5\n",
        "        expected_score_0 = (self.parent_weight * parent_score_0) + (self.child_weight * child_score_0) + (self.structure_weight * structure_score_0)\n",
        "\n",
        "        # Expected Score for Row 1\n",
        "        parent_score_1 = 1.0; child_score_1 = 0.0; structure_score_1 = 0.0\n",
        "        expected_score_1 = (self.parent_weight * parent_score_1) + (self.child_weight * child_score_1) + (self.structure_weight * structure_score_1)\n",
        "\n",
        "        self.assertEqual(len(result), 2)\n",
        "        self.assertAlmostEqual(result[0], expected_score_0, places=3)\n",
        "        self.assertAlmostEqual(result[1], expected_score_1, places=3)\n",
        "\n",
        "\n",
        "# --- Run the tests in the notebook ---\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPGZSHCcKhIn"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "\n",
        "class TestTotalScoreCalculation(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Define weights in ONE place. All tests will use this.\"\"\"\n",
        "        self.weights = {\n",
        "            'date': 0.2,\n",
        "            'place': 0.4,\n",
        "            'variable': 0.4\n",
        "        }\n",
        "\n",
        "    def test_all_scores_present(self):\n",
        "        \"\"\"Test calculation when all component scores are valid.\"\"\"\n",
        "        scores_dict = {'date': 1.0, 'place': 0.8, 'variable': 0.9}\n",
        "\n",
        "        # The test now calculates the expected value automatically\n",
        "        expected_score = 0\n",
        "        for component, weight in self.weights.items():\n",
        "            expected_score += scores_dict[component] * weight\n",
        "\n",
        "        result = calculate_total_scores(\n",
        "            pd.Series([scores_dict['date']]),\n",
        "            pd.Series([scores_dict['place']]),\n",
        "            pd.Series([scores_dict['variable']]),\n",
        "        )\n",
        "        self.assertAlmostEqual(result[0], expected_score)\n",
        "\n",
        "    def test_one_score_is_nan(self):\n",
        "        \"\"\"Test re-normalization when one score is NaN.\"\"\"\n",
        "        scores_dict = {'date': np.nan, 'place': 0.8, 'variable': 0.9}\n",
        "\n",
        "        expected_score, total_weight = 0, 0\n",
        "        for component, score in scores_dict.items():\n",
        "          if pd.notna(score):\n",
        "              expected_score += score * self.weights[component]\n",
        "              total_weight += self.weights[component]\n",
        "        expected_score /= total_weight\n",
        "\n",
        "        result = calculate_total_scores(\n",
        "            pd.Series([scores_dict['date']]),\n",
        "            pd.Series([scores_dict['place']]),\n",
        "            pd.Series([scores_dict['variable']]),\n",
        "        )\n",
        "        self.assertAlmostEqual(result[0], expected_score)\n",
        "\n",
        "    def test_two_scores_are_nan(self):\n",
        "        \"\"\"Test calculation when only one score is valid.\"\"\"\n",
        "        scores_dict = {'date': np.nan, 'place': 0.8, 'variable': np.nan}\n",
        "\n",
        "        expected_score, total_weight = 0, 0\n",
        "        for component, score in scores_dict.items():\n",
        "          if pd.notna(score):\n",
        "              expected_score += score * self.weights[component]\n",
        "              total_weight += self.weights[component]\n",
        "        expected_score /= total_weight\n",
        "\n",
        "        result = calculate_total_scores(\n",
        "            pd.Series([scores_dict['date']]),\n",
        "            pd.Series([scores_dict['place']]),\n",
        "            pd.Series([scores_dict['variable']]),\n",
        "        )\n",
        "        self.assertAlmostEqual(result[0], expected_score)\n",
        "\n",
        "    def test_all_scores_are_nan(self):\n",
        "        \"\"\"Test that the result is NaN when all inputs are NaN.\"\"\"\n",
        "        scores_dict = {'date': np.nan, 'place': np.nan, 'variable': np.nan}\n",
        "\n",
        "        result = calculate_total_scores(\n",
        "            pd.Series([scores_dict['date']]),\n",
        "            pd.Series([scores_dict['place']]),\n",
        "            pd.Series([scores_dict['variable']]),\n",
        "        )\n",
        "        self.assertTrue(np.isnan(result[0]))\n",
        "\n",
        "    def test_vectorized_multi_row(self):\n",
        "        \"\"\"Test a multi-row Series to ensure vectorization works correctly.\"\"\"\n",
        "        # Define scores for multiple rows\n",
        "        scores_data = [\n",
        "            {'date': 1.0, 'place': 0.8, 'variable': 0.9},    # Row 0\n",
        "            {'date': np.nan, 'place': 0.8, 'variable': 0.9}, # Row 1\n",
        "            {'date': 0.5, 'place': 0.7, 'variable': np.nan},  # Row 2\n",
        "            {'date': np.nan, 'place': np.nan, 'variable': np.nan} # Row 3\n",
        "        ]\n",
        "\n",
        "        # Create the input Series\n",
        "        date_s = pd.Series([d['date'] for d in scores_data])\n",
        "        place_s = pd.Series([d['place'] for d in scores_data])\n",
        "        variable_s = pd.Series([d['variable'] for d in scores_data])\n",
        "\n",
        "        # Calculate all expected scores in a loop using the simple oracle\n",
        "        def calculate_weighted_average_for_test(scores, weights):\n",
        "          total_score, total_weight = 0, 0\n",
        "          for component, score in scores.items():\n",
        "            if pd.notna(score):\n",
        "                total_score += score * weights[component]\n",
        "                total_weight += weights[component]\n",
        "          if not total_weight:\n",
        "            return np.nan\n",
        "          return total_score / total_weight\n",
        "\n",
        "        expected_scores = [calculate_weighted_average_for_test(d, self.weights) for d in scores_data]\n",
        "\n",
        "        # Calculate all actual scores at once using the vectorized function\n",
        "        result = calculate_total_scores(date_s, place_s, variable_s)\n",
        "\n",
        "        # Assert each result matches its expectation\n",
        "        self.assertEqual(len(result), 4)\n",
        "        self.assertAlmostEqual(result[0], expected_scores[0])\n",
        "        self.assertAlmostEqual(result[1], expected_scores[1])\n",
        "        self.assertAlmostEqual(result[2], expected_scores[2])\n",
        "        self.assertTrue(np.isnan(result[3]))\n",
        "\n",
        "# --- Run the tests in the notebook ---\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Idp088lPo0w"
      },
      "outputs": [],
      "source": [
        "df = gcs_csv_to_df(goldens_path(golden_epoch), NlGolden)\n",
        "df['variables'].str.len()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fVVnDkcxUCH"
      },
      "source": [
        "### sample set for scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flDiWZzx2hDd"
      },
      "outputs": [],
      "source": [
        "watch_list = [\n",
        "    \"Which cities have the most affordable housing\",\n",
        "    \"incarceration rate by race in US\",\n",
        "    \"How much have gas prices increased in the US since 2000\",\n",
        "              \"List out counties with the worst AQI\",\n",
        "              \"I want to see stats about farming in Sonoma County\",\n",
        "              \"Most common allergies in the US\",\n",
        "              \"Which countries have universal health care\",\n",
        "              \"What is the impact of pollution on ocean life in Tulum\",\n",
        "              \"Areas with the highest crime rate\",\n",
        "    \"How much have gas prices increased in the US since 2000\",\n",
        "    \"Population of the us\"]\n",
        "scrapes = []\n",
        "for query in watch_list:\n",
        "  s = scrape_query(0, query)\n",
        "  scrapes.append(s)\n",
        "scrapes"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
