# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Build the embeddings index by concatenating various inputs."""

# TODO: Consider adding the model name to the embeddings file for downstream
# validation.

import csv
from dataclasses import dataclass
import datetime as datetime
import glob
import os
from typing import Any, Dict, List, Set, Tuple

from absl import app
from absl import flags
from google.cloud import storage
import gspread
import pandas as pd
from sentence_transformers import SentenceTransformer

FLAGS = flags.FLAGS

flags.DEFINE_string('model_name_v2', 'all-MiniLM-L6-v2', 'Model name')
flags.DEFINE_string('bucket_name_v2', 'datcom-nl-models', 'Storage bucket')
flags.DEFINE_string('embeddings_size', '',
                    'Valid values are: "small", "medium"')

flags.DEFINE_string('local_sheets_csv_filepath',
                    'data/curated_input/sheets_svs.csv',
                    'Local Sheets csv (relative) file path')
flags.DEFINE_string(
    'sheets_url',
    'https://docs.google.com/spreadsheets/d/1-QPDWqD131LcDTZ4y_nnqllh66W010HDdows1phyneU',
    'Google Sheets Url for the latest SVs')
flags.DEFINE_string('worksheet_name', 'Demo_SVs',
                    'Worksheet name in the Google Sheets file')

flags.DEFINE_string(
    'autogen_input_basedir', 'data/autogen_input',
    'Base path for CSVs with autogenerated SVs with name and description. '
    'The actual path is `{--autogen_input_base}/{--embeddings_size}/*.csv`.')

flags.DEFINE_string('alternatives_filepattern', 'data/alternatives/*.csv',
                    'File pattern (relative) for CSVs with alternatives')

#
# curated_input/ + autogen_input/ + alternatives/ => preindex/ => embeddings
#

# Col names in the input files/sheets.
DCID_COL = 'dcid'

SHEETS_NAME_COL = 'Name'
SHEETS_DESCRIPTION_COL = 'Description'
SHEETS_ALTERNATIVES_COL = 'Curated_Alternatives'
SHEETS_OVERRIDE_COL = 'Override_Alternatives'

CSV_ALTERNATIVES_COL = 'Alternatives'

# Col names in the concatenated dataframe.
COL_ALTERNATIVES = 'sentence'

# Setting to a very high number right for now.
MAX_ALTERNATIVES_LIMIT = 50


def _add_sv(name: str, sv: str, text2sv: Dict[str, str],
            dup_svs: List[List[str]]) -> None:
  osv = text2sv.get(name)
  if not osv or osv == sv:
    text2sv[name] = sv
    return

  # This is a case of duplicate SV.  Prefer the human-curated, shorter SV.
  # Track it.
  pref, drop = sv, osv
  if ((osv.startswith('dc/') and sv.startswith('dc/')) or
      (not osv.startswith('dc/') and not sv.startswith('dc/'))):
    # Both SVs are autogen or both aren't. Go by dcid len.
    if len(osv) <= len(sv):
      pref, drop = osv, sv
  elif sv.startswith('dc/'):
    # sv is autogen, prefer osv.
    pref, drop = osv, sv

  text2sv[name] = pref
  dup_svs.append([pref, drop, name])


def _dedup_texts(df: pd.DataFrame) -> Tuple[Dict[str, str], List[List[str]]]:
  """Dedup multiple texts mapped to the same DCID and return a list."""
  text2sv_dict = {}
  dup_sv_rows = [['PreferredSV', 'DroppedSV', 'DuplicateName']]
  for _, row in df.iterrows():
    sv = row[DCID_COL].strip()

    # All alternative sentences are retrieved from COL_ALTERNATIVES, which
    # are expected to be delimited by ";" (semi-colon).
    if COL_ALTERNATIVES in row:
      alternatives = row[COL_ALTERNATIVES].split(';')
      alternatives = [a.strip() for a in alternatives if a.strip()]
      for alt in alternatives:
        _add_sv(alt, sv, text2sv_dict, dup_sv_rows)

  return (text2sv_dict, dup_sv_rows)


def _get_texts_dcids(
    text2sv_dict: Dict[str, str]) -> Tuple[List[str], List[str]]:
  texts = sorted(list(text2sv_dict.keys()))
  dcids = [text2sv_dict[k] for k in texts]
  return (texts, dcids)


def _two_digits(number: int) -> str:
  return str(number).zfill(2)


def _make_gcs_embeddings_filename(embeddings_size: str) -> str:
  now = datetime.datetime.now()

  month_str = _two_digits(now.month)
  day_str = _two_digits(now.day)
  hour_str = _two_digits(now.hour)
  minute_str = _two_digits(now.minute)
  second_str = _two_digits(now.second)

  return f"embeddings_{embeddings_size}_{now.year}_{month_str}_{day_str}_{hour_str}_{minute_str}_{second_str}.csv"


def _merge_dataframes(df_1: pd.DataFrame, df_2: pd.DataFrame) -> pd.DataFrame:
  # In case there is a column (besides DCID_COL) which is common, the merged copy
  # will contain two columns (one with a postfix _x and one with a postfix _y.
  # Concatenate the two to produce a final version.
  df_1 = df_1.merge(df_2, how='left', on=DCID_COL,
                    suffixes=("_x", "_y")).fillna("")

  # Determine the columns which were common.
  common_cols = set()
  for col in df_1.columns:
    if col.endswith("_x") or col.endswith("_y"):
      common_cols.add(col.replace("_x", "").replace("_y", ""))

  # Replace the common columns with their concatenation.
  for col in common_cols:
    df_1[col] = df_1[f"{col}_x"].str.cat(df_1[f"{col}_y"], sep=";")
    df_1[col] = df_1[col].replace(to_replace="^;", value="", regex=True)
    df_1 = df_1.drop(columns=[f"{col}_x", f"{col}_y"])

  return df_1


def _concat_alternatives(alternatives: List[str],
                         max_alternatives,
                         delimiter=";") -> str:
  alts = set(alternatives[0:max_alternatives])
  return f"{delimiter}".join(sorted(alts))


def _split_alt_string(alt_string: str) -> List[str]:
  alts = []
  for alt in alt_string.split(";"):
    if alt:
      alts.append(alt.strip())
  return alts


def _build_embeddings(ctx, texts: List[str], dcids: List[str]) -> pd.DataFrame:
  assert len(texts) == len(dcids)

  embeddings = ctx.model.encode(texts, show_progress_bar=True)
  embeddings = pd.DataFrame(embeddings)
  embeddings[DCID_COL] = dcids
  embeddings[COL_ALTERNATIVES] = texts
  return embeddings


def _validateEmbeddings(embeddings_df: pd.DataFrame,
                        output_dcid_sentences_filepath: str) -> None:
  # Verify that embeddings were created for all DCIDs and Sentences.
  dcid_sentence_df = pd.read_csv(output_dcid_sentences_filepath).fillna("")
  sentences = set()
  for alts in dcid_sentence_df["sentence"].values:
    for s in alts.split(";"):
      s = s.strip()
      if not s:
        continue
      sentences.add(s)

  # Verify that each of the texts in the embeddings_df is in the sentences set
  # and that all the sentences in the set are in the embeddings_df. Finally, also
  # verify that embeddings_df has no duplicate sentences.
  embeddings_sentences = embeddings_df['sentence'].values
  embeddings_sentences_unique = set()
  for s in embeddings_sentences:
    assert s in sentences, f"Embeddings sentence not found in processed output file. Sentence: {s}"
    assert s not in embeddings_sentences_unique, f"Found multiple instances of sentence in embeddings. Sentence: {s}."
    embeddings_sentences_unique.add(s)

  for s in sentences:
    assert s in embeddings_sentences_unique, f"Output File sentence not found in Embeddings. Sentence: {s}"

  # Verify that the number of columns = length of the embeddings vector + one each for the
  # dcid and sentence columns.
  assert len(embeddings_df.columns), 384 + 2


def get_sheets_data(ctx, sheets_url: str, worksheet_name: str) -> pd.DataFrame:
  sheet = ctx.gs.open_by_url(sheets_url).worksheet(worksheet_name)
  df = pd.DataFrame(sheet.get_all_records()).fillna("")
  return df


def get_local_alternatives(local_filename: str,
                           local_col_names: List[str]) -> pd.DataFrame:
  df = pd.read_csv(local_filename).fillna("")
  df = df[local_col_names]
  return df


def _write_intermediate_output(name2sv_dict: Dict[str, str],
                               dup_sv_rows: List[List[str]],
                               local_merged_filepath: str,
                               dup_names_filepath: str) -> None:
  sv2names = {}
  for name, sv in name2sv_dict.items():
    if sv not in sv2names:
      sv2names[sv] = []
    sv2names[sv].append(name)

  sv_list = sorted(list(sv2names.keys()))
  name_list = [';'.join(sorted(sv2names[v])) for v in sv_list]

  # Write to local_merged_filepath.
  print(
      f"Writing the concatenated dataframe after merging alternates to local file: {local_merged_filepath}"
  )
  df_svs = pd.DataFrame({'dcid': sv_list, 'sentence': name_list})
  df_svs.to_csv(local_merged_filepath, index=False)

  if dup_names_filepath:
    print(f"Writing duplicate names file: {dup_names_filepath}")
    with open(dup_names_filepath, 'w') as f:
      csv.writer(f).writerows(dup_sv_rows)


def get_embeddings(ctx, df_svs: pd.DataFrame, local_merged_filepath: str,
                   dup_names_filepath: str) -> pd.DataFrame:
  print(f"Concatenate all alternative sentences for descriptions.")
  alternate_descriptions = []
  for _, row in df_svs.iterrows():
    alternatives = []
    if row[SHEETS_OVERRIDE_COL]:
      # Override takes precendence over everything else.
      alternatives += _split_alt_string(row[SHEETS_OVERRIDE_COL])
    else:
      for col_name in [
          SHEETS_NAME_COL,
          SHEETS_DESCRIPTION_COL,
          SHEETS_ALTERNATIVES_COL,
          CSV_ALTERNATIVES_COL,
      ]:
        # In order of preference, traverse the various alternative descriptions.
        alternatives += _split_alt_string(row[col_name])

    alt_str = _concat_alternatives(alternatives, MAX_ALTERNATIVES_LIMIT)
    alternate_descriptions.append(alt_str)

  assert len(df_svs) == len(alternate_descriptions)
  df_svs[COL_ALTERNATIVES] = alternate_descriptions
  # Trim df
  df_svs = df_svs[[DCID_COL, COL_ALTERNATIVES]]

  # Dedupe texts
  (name2sv_dict, dup_sv_rows) = _dedup_texts(df_svs)

  # Write dcid -> texts and dups to intermediate files.
  _write_intermediate_output(name2sv_dict, dup_sv_rows, local_merged_filepath,
                             dup_names_filepath)

  print("Getting texts, dcids and embeddings.")
  (texts, dcids) = _get_texts_dcids(name2sv_dict)

  print("Building embeddings")
  return _build_embeddings(ctx, texts, dcids)


def build(ctx, sheets_url: str, worksheet_name: str,
          local_sheets_csv_filepath: str, local_merged_filepath: str,
          dup_names_filepath: str, autogen_input_filepattern: str,
          alternative_filepattern: str) -> pd.DataFrame:
  # First download the latest file from sheets.
  print(
      f"Downloading the latest sheets data from: {sheets_url} (worksheet: {worksheet_name})"
  )
  df_svs = get_sheets_data(ctx, sheets_url, worksheet_name)
  print(f"Downloaded {len(df_svs)} rows and {len(df_svs.columns)} columns.")

  # Write this downloaded file to local.
  print(
      f"Writing the downloaded dataframe to local at: {local_sheets_csv_filepath}"
  )
  df_svs.to_csv(local_sheets_csv_filepath, index=False)

  # Append autogen CSVs if any.
  autogen_dfs = []
  for autogen_csv in sorted(glob.glob(autogen_input_filepattern)):
    print(f'Processing autogen input file: {autogen_csv}')
    autogen_dfs.append(pd.read_csv(autogen_csv).fillna(""))
  if autogen_dfs:
    df_svs = pd.concat([df_svs] + autogen_dfs)
    df_svs = df_svs.drop_duplicates(subset=DCID_COL)

  # Get alternatives and add to the dataframe.
  for alt_fp in sorted(glob.glob(alternative_filepattern)):
    df_alts = get_local_alternatives(alt_fp, [DCID_COL, CSV_ALTERNATIVES_COL])
    df_svs = _merge_dataframes(df_svs, df_alts)

  return get_embeddings(ctx, df_svs, local_merged_filepath, dup_names_filepath)


@dataclass
class Context:
  # gspread client
  gs: Any
  # Model
  model: Any
  # GCS storage bucket
  bucket: Any
  # Temp dir
  tmp: str


def main(_):
  assert FLAGS.model_name_v2 and FLAGS.bucket_name_v2 and FLAGS.local_sheets_csv_filepath and FLAGS.sheets_url and FLAGS.worksheet_name
  assert FLAGS.embeddings_size in ['small', 'medium']

  assert os.path.exists(os.path.join('data'))

  local_merged_filepath = f'data/preindex/{FLAGS.embeddings_size}/sv_descriptions.csv'
  dup_names_filepath = f'data/preindex/{FLAGS.embeddings_size}/duplicate_names.csv'
  autogen_input_filepattern = f'{FLAGS.autogen_input_basedir}/{FLAGS.embeddings_size}/*.csv'

  gs = gspread.oauth()
  sc = storage.Client()
  bucket = sc.bucket(FLAGS.bucket_name_v2)
  model = SentenceTransformer(FLAGS.model_name_v2)

  ctx = Context(gs=gs, model=model, bucket=bucket, tmp='/tmp')

  gcs_embeddings_filename = _make_gcs_embeddings_filename(FLAGS.embeddings_size)
  gcs_tmp_out_path = os.path.join(ctx.tmp, gcs_embeddings_filename)

  # Process all the data, produce the final dataframes, build the embeddings and return the embeddings dataframe.
  # During this process, the downloaded latest SVs and Descriptions data and the
  # final dataframe with SVs and Alternates are also written to local_merged_dir.
  embeddings_df = build(ctx, FLAGS.sheets_url, FLAGS.worksheet_name,
                        FLAGS.local_sheets_csv_filepath, local_merged_filepath,
                        dup_names_filepath, autogen_input_filepattern,
                        FLAGS.alternatives_filepattern)

  print(f"Saving locally to {gcs_tmp_out_path}")
  embeddings_df.to_csv(gcs_tmp_out_path, index=False)

  # Before uploading embeddings to GCS, validate them.
  print("Validating the built embeddings.")
  _validateEmbeddings(embeddings_df, local_merged_filepath)
  print("Embeddings DataFrame is validated.")

  # Finally, upload to the NL embeddings server's GCS bucket
  print("Attempting to write to GCS")
  print(f"\t GCS Path: gs://{FLAGS.bucket_name_v2}/{gcs_embeddings_filename}")
  blob = ctx.bucket.blob(gcs_embeddings_filename)
  blob.upload_from_filename(gcs_tmp_out_path)
  print("Done uploading to gcs.")
  print(f"\t Embeddings Filename: {gcs_embeddings_filename}")
  print("\nNOTE: Please update model.yaml with the Embeddings Filename")


if __name__ == "__main__":
  app.run(main)
