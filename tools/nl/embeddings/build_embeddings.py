# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Build the embeddings index by concatenating various inputs."""

# TODO: Consider adding the model name to the embeddings file for downstream
# validation.

import csv
import datetime as datetime
import glob
import os
from typing import Dict, List

from absl import app
from absl import flags
from google.cloud import storage
import gspread
import pandas as pd
from sentence_transformers import SentenceTransformer
import utils

FLAGS = flags.FLAGS

# TODO: use only one flag from the two below and "gcs://" prefix to differentiate
# between local and GCS path.
flags.DEFINE_string('finetuned_model_gcs', '',
                    'Existing finetuned model folder name on GCS')
flags.DEFINE_string('existing_model_path', '',
                    'Path to an existing model (local)')
flags.DEFINE_string('model_name_v2', 'all-MiniLM-L6-v2', 'Model name')
flags.DEFINE_string('bucket_name_v2', 'datcom-nl-models', 'Storage bucket')
flags.DEFINE_string('embeddings_size', '', 'Embeddings size')

flags.DEFINE_string('curated_input_path',
                    'data/curated_input/main/sheets_svs.csv',
                    'Curated input csv (relative) file path')

flags.DEFINE_string(
    'autogen_input_basedir', 'data/autogen_input',
    'Base path for CSVs with autogenerated SVs with name and description. '
    'The actual path is `{--autogen_input_base}/{--embeddings_size}/*.csv`.')

flags.DEFINE_string('alternatives_filepattern', 'data/alternatives/main/*.csv',
                    'File pattern (relative) for CSVs with alternatives')

#
# curated_input/ + autogen_input/ + alternatives/ => preindex/ => embeddings
#

# Setting to a very high number right for now.
MAX_ALTERNATIVES_LIMIT = 50


def _make_gcs_embeddings_filename(embeddings_size: str,
                                  model_version: str) -> str:
  now = datetime.datetime.now()

  month_str = utils.two_digits(now.month)
  day_str = utils.two_digits(now.day)
  hour_str = utils.two_digits(now.hour)
  minute_str = utils.two_digits(now.minute)
  second_str = utils.two_digits(now.second)

  return f"embeddings_{embeddings_size}_{now.year}_{month_str}_{day_str}_{hour_str}_{minute_str}_{second_str}.{model_version}.csv"


def _write_intermediate_output(name2sv_dict: Dict[str, str],
                               dup_sv_rows: List[List[str]],
                               local_merged_filepath: str,
                               dup_names_filepath: str) -> None:
  sv2names = {}
  for name, sv in name2sv_dict.items():
    if sv not in sv2names:
      sv2names[sv] = []
    sv2names[sv].append(name)

  sv_list = sorted(list(sv2names.keys()))
  name_list = [';'.join(sorted(sv2names[v])) for v in sv_list]

  # Write to local_merged_filepath.
  print(
      f"Writing the concatenated dataframe after merging alternates to local file: {local_merged_filepath}"
  )
  df_svs = pd.DataFrame({'dcid': sv_list, 'sentence': name_list})
  df_svs.to_csv(local_merged_filepath, index=False)

  if dup_names_filepath:
    print(f"Writing duplicate names file: {dup_names_filepath}")
    with open(dup_names_filepath, 'w') as f:
      csv.writer(f).writerows(dup_sv_rows)


def get_embeddings(ctx, df_svs: pd.DataFrame, local_merged_filepath: str,
                   dup_names_filepath: str) -> pd.DataFrame:
  print(f"Concatenate all alternative sentences for descriptions.")
  alternate_descriptions = []
  for _, row in df_svs.iterrows():
    alternatives = []
    if row[utils.OVERRIDE_COL]:
      # Override takes precendence over everything else.
      alternatives += utils.split_alt_string(row[utils.OVERRIDE_COL])
    else:
      for col_name in [
          utils.NAME_COL,
          utils.DESCRIPTION_COL,
          utils.CURATED_ALTERNATIVES_COL,
          utils.ALTERNATIVES_COL,
      ]:
        # In order of preference, traverse the various alternative descriptions.
        alternatives += utils.split_alt_string(row[col_name])

    alt_str = utils.concat_alternatives(alternatives, MAX_ALTERNATIVES_LIMIT)
    alternate_descriptions.append(alt_str)

  assert len(df_svs) == len(alternate_descriptions)
  df_svs[utils.COL_ALTERNATIVES] = alternate_descriptions
  # Trim df
  df_svs = df_svs[[utils.DCID_COL, utils.COL_ALTERNATIVES]]

  # Dedupe texts
  (name2sv_dict, dup_sv_rows) = utils.dedup_texts(df_svs)

  # Write dcid -> texts and dups to intermediate files.
  _write_intermediate_output(name2sv_dict, dup_sv_rows, local_merged_filepath,
                             dup_names_filepath)

  print("Getting texts, dcids and embeddings.")
  (texts, dcids) = utils.get_texts_dcids(name2sv_dict)

  print("Building embeddings")
  return utils.build_embeddings(ctx, texts, dcids)


def build(ctx, curated_input_path: str, local_merged_filepath: str,
          dup_names_filepath: str, autogen_input_filepattern: str,
          alternative_filepattern: str) -> pd.DataFrame:
  # Read curated sv info.
  try:
    print(f"Reading the curated input file: {curated_input_path}")
    df_svs = pd.read_csv(curated_input_path)
  except:
    print(
        "Error reading curated input file. Continuing with an empty dataframe.")
    df_svs = pd.DataFrame()

  # Append autogen CSVs if any.
  autogen_dfs = []
  for autogen_csv in sorted(glob.glob(autogen_input_filepattern)):
    print(f'Processing autogen input file: {autogen_csv}')
    autogen_dfs.append(pd.read_csv(autogen_csv).fillna(""))
  if autogen_dfs:
    df_svs = pd.concat([df_svs] + autogen_dfs)
    df_svs = df_svs.drop_duplicates(subset=utils.DCID_COL)

  # Get alternatives and add to the dataframe.
  if alternative_filepattern:
    for alt_fp in sorted(glob.glob(alternative_filepattern)):
      df_alts = utils.get_local_alternatives(
          alt_fp, [utils.DCID_COL, utils.ALTERNATIVES_COL])
      df_svs = utils.merge_dataframes(df_svs, df_alts)

  return get_embeddings(ctx, df_svs, local_merged_filepath, dup_names_filepath)


def main(_):
  assert FLAGS.model_name_v2 and FLAGS.bucket_name_v2 and FLAGS.curated_input_path

  assert os.path.exists(os.path.join('data'))

  if FLAGS.existing_model_path:
    assert os.path.exists(FLAGS.existing_model_path)

  use_finetuned_model = False
  use_local_model = False
  model_version = FLAGS.model_name_v2
  if FLAGS.finetuned_model_gcs:
    use_finetuned_model = True
    model_version = FLAGS.finetuned_model_gcs
  elif FLAGS.existing_model_path:
    use_local_model = True
    model_version = os.path.basename(FLAGS.existing_model_path)

  if not os.path.exists(os.path.join('data', 'preindex',
                                     FLAGS.embeddings_size)):
    os.mkdir(os.path.join('data', 'preindex', FLAGS.embeddings_size))
  local_merged_filepath = f'data/preindex/{FLAGS.embeddings_size}/sv_descriptions.csv'
  dup_names_filepath = f'data/preindex/{FLAGS.embeddings_size}/duplicate_names.csv'

  if not os.path.exists(
      os.path.join(FLAGS.autogen_input_basedir, FLAGS.embeddings_size)):
    os.mkdir(os.path.join(FLAGS.autogen_input_basedir, FLAGS.embeddings_size))
  autogen_input_filepattern = f'{FLAGS.autogen_input_basedir}/{FLAGS.embeddings_size}/*.csv'

  gs = gspread.oauth()
  sc = storage.Client()
  bucket = sc.bucket(FLAGS.bucket_name_v2)

  if use_finetuned_model:
    ctx_no_model = utils.Context(gs=gs, model=None, bucket=bucket, tmp='/tmp')
    model = utils.get_ft_model_from_gcs(ctx_no_model, model_version)

  elif use_local_model:
    print(f"Use the local model at: {FLAGS.existing_model_path}")
    print(f"Extracted model version: {model_version}")
    model = SentenceTransformer(FLAGS.existing_model_path)

  else:
    model = SentenceTransformer(FLAGS.model_name_v2)

  ctx = utils.Context(gs=gs, model=model, bucket=bucket, tmp='/tmp')

  gcs_embeddings_filename = _make_gcs_embeddings_filename(
      FLAGS.embeddings_size, model_version)
  gcs_tmp_out_path = os.path.join(ctx.tmp, gcs_embeddings_filename)

  # Process all the data, produce the final dataframes, build the embeddings and return the embeddings dataframe.
  # During this process, the downloaded latest SVs and Descriptions data and the
  # final dataframe with SVs and Alternates are also written to local_merged_dir.
  embeddings_df = build(ctx, FLAGS.curated_input_path, local_merged_filepath,
                        dup_names_filepath, autogen_input_filepattern,
                        FLAGS.alternatives_filepattern)

  print(f"Saving locally to {gcs_tmp_out_path}")
  embeddings_df.to_csv(gcs_tmp_out_path, index=False)

  # Before uploading embeddings to GCS, validate them.
  print("Validating the built embeddings.")
  utils.validate_embeddings(embeddings_df, local_merged_filepath)
  print("Embeddings DataFrame is validated.")

  # Finally, upload to the NL embeddings server's GCS bucket
  print("Attempting to write to GCS")
  print(f"\t GCS Path: gs://{FLAGS.bucket_name_v2}/{gcs_embeddings_filename}")
  blob = ctx.bucket.blob(gcs_embeddings_filename)
  # Since the files can be fairly large, use a 10min timeout to be safe.
  blob.upload_from_filename(gcs_tmp_out_path, timeout=600)
  print("Done uploading to gcs.")
  print(f"\t Embeddings Filename: {gcs_embeddings_filename}")
  print("\nNOTE: Please update embeddings.yaml with the Embeddings Filename")


if __name__ == "__main__":
  app.run(main)
