# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Fine Tune the base model."""

from datetime import datetime
import glob
import os
import time
from typing import Any, List

from absl import app
from absl import flags
from google.cloud import storage
import gspread
import pandas as pd
from sentence_transformers import InputExample
from sentence_transformers import losses
from sentence_transformers import SentenceTransformer
from torch.utils.data import DataLoader
import utils

FLAGS = flags.FLAGS

flags.DEFINE_string('model_name_v2', 'all-MiniLM-L6-v2', 'Model name')
flags.DEFINE_string('bucket_name_v2', 'datcom-nl-models', 'Storage bucket')
flags.DEFINE_string('embeddings_size', '',
                    'Valid values are: "small", "medium"')

flags.DEFINE_string(
    'autogen_input_basedir', 'data/autogen_input',
    'Base path for CSVs with autogenerated SVs with name and description. '
    'The actual path is `{--autogen_input_base}/{--embeddings_size}/*.csv`.')

flags.DEFINE_string('alternatives_filepattern', 'data/alternatives/*.csv',
                    'File pattern (relative) for CSVs with alternatives')

flags.DEFINE_string('sentence_pairs_filepath',
                    'data/finetuning/sentence_pairs.csv',
                    'File path for sentence pairs CSV')

BATCH_SIZE = 256
NUM_WARMUP_STEPS = 10
NUM_EPOCHS = 10

VERY_HIGH_MATCH_SCORE = 0.95
HIGH_MATCH_SCORE = 0.90
MEDIUM_HIGH_MATCH_SCORE = 0.85


def _upload_to_gcs(ctx: utils.Context,
                   gcs_path: str,
                   local_filepath: str,
                   empty_folder: bool = False) -> None:
  print(f'Uploading {local_filepath}')
  blob = ctx.bucket.blob(gcs_path)

  if empty_folder:
    blob.upload_from_string(
        '', content_type='application/x-www-form-urlencoded;charset=UTF-8')
  else:
    blob.upload_from_filename(local_filepath)
  print(f'Path in GCS: {gcs_path}')


def _make_gcs_model_folder(base_model_name: str) -> str:
  now = datetime.now()

  month_str = utils.two_digits(now.month)
  day_str = utils.two_digits(now.day)
  hour_str = utils.two_digits(now.hour)
  minute_str = utils.two_digits(now.minute)
  second_str = utils.two_digits(now.second)

  return f"model_{base_model_name}_{now.year}_{month_str}_{day_str}_{hour_str}_{minute_str}_{second_str}"


def _alternatives(autogen_input_filepattern: str,
                  alternatives_filepattern: str) -> pd.DataFrame:

  df_svs = pd.DataFrame()
  # Append autogen CSVs if any.
  autogen_dfs = []
  for autogen_csv in sorted(glob.glob(autogen_input_filepattern)):
    print(f'Processing autogen input file: {autogen_csv}')
    autogen_dfs.append(pd.read_csv(autogen_csv).fillna(""))
  if autogen_dfs:
    df_svs = pd.concat([df_svs] + autogen_dfs)
    df_svs = df_svs.drop_duplicates(subset=utils.DCID_COL)

  # Get alternatives and add to the dataframe.
  for alt_fp in sorted(glob.glob(alternatives_filepattern)):
    df_alts = utils.get_local_alternatives(
        alt_fp, [utils.DCID_COL, utils.ALTERNATIVES_COL])
    df_svs = utils.merge_dataframes(df_svs, df_alts)

  return df_svs


def _save_finetuned_model(ctx: utils.Context, gcs_tmp_out_path: str,
                          gcs_model_folder: str) -> None:
  # To upload the model directory, we need to traverse the files and folders.
  for str_path in glob.glob(f"{gcs_tmp_out_path}/**"):
    # Check if str_path is a folder.
    if os.path.isdir(str_path):
      if not glob.glob(f"{str_path}/**"):
        # This means we found an empty folder.
        foldername = os.path.basename(str_path)
        gcs_path = gcs_model_folder + "/" + foldername + "/"
        print(f'Path in GCS: {gcs_path}')
        _upload_to_gcs(ctx, gcs_path, str_path, empty_folder=True)

      for filepath in glob.glob(f"{str_path}/**"):
        # Found files under a folder.
        filename = filepath.split(gcs_tmp_out_path)[1]
        gcs_path = gcs_model_folder + filename
        _upload_to_gcs(ctx, gcs_path, filepath)
    else:
      # Just files under the main model folder.
      foldername = os.path.basename(str_path)
      gcs_path = gcs_model_folder + "/" + foldername
      _upload_to_gcs(ctx, gcs_path, str_path)


def fine_tune_model(model: Any, df_svs: pd.DataFrame,
                    df_sentence_pairs: pd.DataFrame):
  """Fine tuning involves providing pairs of sentences/text with an
  approximate similar score to a baseline model. These pairs (and scores)
  are used for further `training` (with CosineSimilarityLoss as the objecive).
  The end result is that the baseline model's weights get updated based on
  the `new` training examples (pairs).
  Effectively, the training examples (pairs and scores) provide additional
  context to a baseline model about the kinds of associations between
  sentences/text that we care about.
  In this case, we use the StatVar name, description and alternatives
  (human curated and LLM-generated) to create pairs of sentences/text with
  high similarity scores.
  We also provide a way to indicate that some pairs of texts/words/sentences
  should not be associated with each other by assigning very low scores.
  This allows us to bias the model's weights to accomodate our particular
  use case better. 
  """

  training_examples: List[InputExample] = []

  # Iterate over SV Names, Descriptions and Alternatives to produce
  # pairs of texts which we want to associated with each other in
  # terms of similarity.
  for _, row in df_svs.iterrows():
    name = row[utils.NAME_COL]
    descriptions = row[utils.DESCRIPTION_COL].split(";")
    desc = descriptions[0]

    if not name and not desc:
      continue
    if ((not name) and desc):
      name = desc
    elif ((not desc) and name):
      desc = name

    curated = row[utils.CURATED_ALTERNATIVES_COL].split(";")
    palm_alts = row[utils.ALTERNATIVES_COL].split(";")

    # Pair name and description with very high score.
    if name != desc:
      training_examples.append(
          InputExample(texts=[name, desc], label=VERY_HIGH_MATCH_SCORE))

    # If there are more descriptions, pair them with the name as well.
    if len(descriptions) > 1:
      for i in range(1, len(descriptions)):
        if descriptions[i]:
          training_examples.append(
              InputExample(texts=[name, descriptions[i]],
                           label=HIGH_MATCH_SCORE))

    for c in curated:
      # All all curated alternatives as pairs with the description.
      if c:
        training_examples.append(
            InputExample(texts=[desc, c], label=VERY_HIGH_MATCH_SCORE))

    for p in palm_alts:
      if p:
        # High match score but since these are auto-generated, keep the score lower
        # than for the manual/curated cases.
        training_examples.append(
            InputExample(texts=[name, p], label=MEDIUM_HIGH_MATCH_SCORE))

  # Now add the manual sentence pairs (which are assumed to have the scores).
  for _, row in df_sentence_pairs.iterrows():
    assert "sentence_1" in row
    assert "sentence_2" in row
    assert float(row["score"])
    training_examples.append(
        InputExample(texts=[row["sentence_1"], row["sentence_2"]],
                     label=row["score"]))

  # Ready to fine-tune.
  # Setting shuffle to True to ensure the training examples are not always provided
  # in the same manner for each epoch.
  train_dataloader = DataLoader(training_examples,
                                shuffle=True,
                                batch_size=BATCH_SIZE)
  t = time.time()
  model.fit(train_objectives=[(train_dataloader,
                               losses.CosineSimilarityLoss(model=model))],
            epochs=NUM_EPOCHS,
            warmup_steps=NUM_WARMUP_STEPS)
  t = time.time() - t
  print(f"Time taken = {t}")
  return model


def main(_):

  assert FLAGS.model_name_v2 and FLAGS.bucket_name_v2
  assert FLAGS.embeddings_size in ['small', 'medium']

  assert os.path.exists(os.path.join('data'))

  autogen_input_filepattern = f'{FLAGS.autogen_input_basedir}/{FLAGS.embeddings_size}/*.csv'

  gs = gspread.oauth()
  sc = storage.Client()
  bucket = sc.bucket(FLAGS.bucket_name_v2)

  # Step 1. Gather the sentence/text alternatives and load the base model.
  print("Gathering the training sentence/text pairs.")
  df_svs = _alternatives(autogen_input_filepattern,
                         FLAGS.alternatives_filepattern)
  df_sentence_pairs = pd.read_csv(FLAGS.sentence_pairs_filepath).fillna("")
  print(f"Found {len(df_svs)} rows in the alternatives dataframe.")
  print(
      f"Found {len(df_sentence_pairs)} human-curated sentence pairs with scores."
  )
  print(f"Loading the base model: {FLAGS.model_name_v2}")
  model = SentenceTransformer(FLAGS.model_name_v2)

  # Step 2. Fine tuning.
  print("Beginning fine tuning.")
  model = fine_tune_model(model, df_svs, df_sentence_pairs)

  ctx = utils.Context(gs=gs, model=model, bucket=bucket, tmp='/tmp')

  gcs_model_folder = _make_gcs_model_folder(FLAGS.model_name_v2)
  gcs_tmp_out_path = os.path.join(ctx.tmp, gcs_model_folder)

  print(f"Saving finetuned model locally to {gcs_tmp_out_path}")
  model.save(gcs_tmp_out_path)

  # Step 3. Upload the finetuned model to the NL model server's GCS bucket.
  print("Attempting to write to GCS")
  print(f"\t GCS Path: gs://{FLAGS.bucket_name_v2}/{gcs_model_folder}/")

  _save_finetuned_model(ctx, gcs_tmp_out_path, gcs_model_folder)

  print("Done uploading to gcs.")
  print(f"\t Finetuned Model Filename: {gcs_model_folder}")

  # TODO: update the model.yalm file to indicate the finetuned model name.
  # print("\nNOTE: Please update model.yaml with the Finetuned Model Filename")


if __name__ == "__main__":
  app.run(main)
