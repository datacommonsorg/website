# Tools and Data for Stat Var Embeddings Index

This directory contains the data sheets (containing StatVar DCID and
descriptions) and script used to construct the Stat Var Embeddings Index that
is loaded into the NL Server in Website.

There are multiple sizes of indexes: small, medium.

As of May 2023, the small index has ~1.3K variables and medium index has 5.3K
variables.

## Curated Input Google Sheet

Latest sheet as of May 2023 is
[`DC_NL_SVs_Curated`](https://docs.google.com/spreadsheets/d/1-QPDWqD131LcDTZ4y_nnqllh66W010HDdows1phyneU).

This is a common sheet across the different index sizes.

## Making a change to the embeddings.

1. For any carefully curated SVs/topics, make edits to the [latest sheet](https://docs.google.com/spreadsheets/d/1-QPDWqD131LcDTZ4y_nnqllh66W010HDdows1phyneU). The columns in this sheet are:

   * `dcid`: the StatVar DCID.
   * `Name`: the name of the StatVar
   * `Description`: the description(s) of the StatVar. Multiple descriptions are acceptable. If multiple description strings are provided, they must be semi-colon delimited. This column can be expected to be used as part of any automated alternative generation processed, e.g. using an LLM.
   * `Override_Alternatives`: if this column has any value in a row, all other columns are ignored. Multiple strings are acceptable for this column. If multiple strings are provided, they must be semi-colon delimited.
   * `Curated_Alternatives`: a semi-colon delimited list of strings which can serve as alternative ways of referring to the StatVar.

1. Add any auto-generated input CSVs to `data/autogen_input/<index-size>/`.
   This is typically generated by code [here](prep/).

   Note: This is what distinguishes between the differently sized (small, medium) embeddings.

1. Ensure any updated alternatives, i.e. PaLM alternatives, Other alternatives, are available as csv files: [`palm_alternatives.csv`](data/alternatives/palm_alternatives.csv), [`other_alternatives.csv`](data/alternatives/other_alternatives.csv). The columns in these CSV files are: `dcid`, `Alternatives`. These files are expected to be updated using (currently) separate processes.

1. Run the command below which will both generate a new embeddings csv in
   `gs://datcom-nl-models`, as well as update the corresponding csv under
   [`data/curated_input/`](data/curated_input/).  Note down the embeddings
   file version printed at the end of the run.

    ```bash
    ./run.sh (small | medium)
    ```
1. Validate the CSV diffs, update [`model.yaml`](../../../deploy/base/model.yaml) with the generated embeddings version and test out locally.

1. Generate an SV embeddings differ report by following the process under the [`sv_index_differ`](../svindex_differ/README.md) folder (one level up). Look at the diffs and evaluate whether they make sense.

1. If everything looks good, send out a PR with the `model.yaml`, the `differ_report.html` file (as a linked attachement) and CSV changes.

## One time setup

To allow the `gspread` library access to the google sheets above, you will need [credentials downloaded to your computer](https://docs.gspread.org/en/latest/oauth2.html#for-end-users-using-oauth-client-id).

As of Feb 2023, you can download the gspread-python-app credentials [found here](https://pantheon.corp.google.com/apis/credentials/oauthclient/878764285063-2tqmvvstv8k8cdl7ougccd7ptpnat8d5.apps.googleusercontent.com?project=datcom-204919) to `~/.config/gspread/credentials.json`.


# Fine Tuning the Model

To fine tune an existing  `sentence_transform` model, do the following:

```bash
    ./run_finetune.sh (small | medium)
```

This process requires non-empty `.csv` files under [`data/alternatives/`](data/alternatives) and [`data/autogen/*`](data/autogen_input/) from which to pick synonymous pairs of sentences/descriptions. These pairs are formed between StatVar `Name`, `Description` and `Alternatives` (either human curated or LLM-generated). The goal is to fine tune the model by understanding that these pairs are close. Additionally, and perhaps more importantly, the [`finetuning/sentence_pairs.csv`](data/finetuning/sentence_pairs.csv) file contains pairs of sentences and a relative similarity indicator ranging between `[0, 1]` where close to `0` means very dissimilar and close to `1` means very similar. Note that the similarity indicator (`label`) does not need to be exact--it is simply a helpful indicator for the fine tuning processs to refine the model by taking in to account the pairs and their suggested labels (scores). We assign high scores (close to 1) to the pairs generated from the various alternatives files. We use [`finetuning/sentence_pairs.csv`](data/finetuning/sentence_pairs.csv) to get the pairs with low similarity scores. These are essentially the pairs for which we want the model to produce very dissimilar scores.