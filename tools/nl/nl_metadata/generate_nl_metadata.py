# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
This script manages metadata for Data Commons Statistical Variables (SVs), primarily for use in Natural Language (NL) search applications.
It supports several modes of operation for generating, updating, and managing metadata files in Google Cloud Storage (GCS).

RUN MODES:
- bigquery: Generates metadata for a full set of SVs from a BigQuery table. The output is written to a new, timestamped folder in GCS.
  This is intended for full, clean builds of the metadata.

- bigquery_diffs: Compares the SVs in a GCS folder (e.g., a periodic snapshot) against the current SVs in BigQuery. It then generates
  metadata for only the new or changed SVs. The output is written as new 'diff' files into the *same* GCS folder, allowing for
  an additive update workflow. This is the primary mode for periodic updates.

- retry_failures: Processes files from a 'failures' directory (generated by a previous 'bigquery' or 'bigquery_diffs' run) and
  reattempts the Gemini API calls. Successfully processed SVs are written as new 'diff_retry' files back to the parent GCS folder.

- compact: Merges all .jsonl files in the root of a specified GCS folder into a single, new file. This is a utility mode for
  periodically cleaning up the numerous small 'diff' files created by the update process. It can optionally delete the original
  files after a successful compaction.

- nl_only: A legacy mode for processing a small, curated list of SVs for NL search.

WORKFLOW:
The intended workflow for maintaining an up-to-date metadata set in a GCS folder is:
1. Run in 'bigquery_diffs' mode periodically (e.g., weekly) to add new SVs.
2. If failures occur, run in 'retry_failures' mode to reprocess them.
3. Run in 'compact' mode occasionally (e.g., monthly) to merge the accumulated diff files into a single master file.

USAGE:
To run this script, create a .env file from the .env.sample and provide the necessary API keys.
Then, execute the script with the desired runMode and arguments. See the README.md for detailed command examples.
"""
import argparse
import asyncio
from datetime import datetime
import json
import os
import sys

# WARNING: This disables a Python safety feature to allow parsing of extremely
# long integers from the Gemini API response. This is a workaround for cases
# where the model may return unexpectedly large numbers.
sys.set_int_max_str_digits(0)  # 0 means no limit

import config
import data_loader
from dotenv import load_dotenv
from gemini_prompt import get_gemini_prompt
from gemini_service import batch_generate_alt_sentences
from google.cloud import storage
import pandas as pd
from schemas import englishSchema
from schemas import frenchSchema
from schemas import spanishSchema

load_dotenv(dotenv_path=config.DOTENV_FILE_PATH)
DC_API_KEY = os.getenv("DC_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")


def extract_flags() -> argparse.Namespace:
  """
  Defines and extracts the script flags from the command line arguments.
  Note that for boolean flags (--useGCS, and --useBigQuery), if these flags are present in the command line, 
  they will be set to True.
  """
  parser = argparse.ArgumentParser(description="./generate_nl_metadata.py")
  parser.add_argument("--runMode",
                      help="The run mode for the script.",
                      choices=[
                          config.RUN_MODE_NL_ONLY, config.RUN_MODE_BIGQUERY,
                          config.RUN_MODE_RETRY_FAILURES,
                          config.RUN_MODE_BIGQUERY_DIFFS,
                          config.RUN_MODE_COMPACT
                      ],
                      required=True,
                      type=str)
  parser.add_argument(
      "--geminiApiKey",
      help="The Gemini API key to use for generating alternative sentences.",
      type=str,
      default=GEMINI_API_KEY)
  parser.add_argument(
      "--language",
      help=
      "The language to return the metadata results in. Currently supports English, French, and Spanish.",
      choices=["English", "French", "Spanish"],
      type=str,
      default="English")
  parser.add_argument("--useGCS",
                      help="Whether to save results to/read input from GCS.",
                      action="store_true",
                      default=False)
  parser.add_argument(
      "--maxStatVars",
      help=
      "The maximum number of statvars to process from BigQuery. If specified, will limit the number of statvars processed to this number.",
      type=int,
      default=None)
  parser.add_argument(
      "--gcsFolder",
      help=
      "The folder in the GCS bucket to save the results to. If unspecified, will choose one of 'full_statvar_metadata_staging', 'nl_statvar_metadata_staging' or 'statvar_metadata_retries' depending on other user inputs.",
      type=str,
      default=None)
  parser.add_argument(
      "--gcsFailureFolder",
      help=
      "The GCS folder to save failure files to. Defaults to a 'failures' subfolder within the main gcsFolder.",
      type=str,
      default=None)
  parser.add_argument(
      "--totalPartitions",
      help=
      "The total number of partitions to run in parallel, each using a different Gemini API key. Only used if --useBigQuery is specified.",
      type=int,
      default=1)
  parser.add_argument(
      "--currPartition",
      help=
      "The current partition number (0-indexed) to run. Should be within the range [0, totalPartitions). Only used if --useBigQuery is specified.",
      type=int,
      default=0)
  parser.add_argument(
      "--output_filename",
      help=
      "For 'compact' mode: The name for the new, compacted file. Defaults to 'compacted_YYYYMMDD_HHMMSS.jsonl'.",
      type=str,
      default="")
  parser.add_argument(
      "--delete_originals",
      help=
      "For 'compact' mode: If set, deletes the original files after compaction. Defaults to False.",
      action="store_true",
      default=False)
  args = parser.parse_args()
  return args


def verify_gcs_path_exists(gcs_path: str) -> bool:
  """
  Verifies that the GCS path exists.
  Returns True if the path exists, False otherwise.
  """
  gcs_client = storage.Client(project=config.GCS_PROJECT_ID)
  bucket = gcs_client.bucket(config.GCS_BUCKET)

  if gcs_path.endswith('.json'):  # Path is a file
    blob = bucket.blob(gcs_path)
    return blob.exists()
  else:  # Path is a folder
    blobs = list(bucket.list_blobs(prefix=gcs_path, max_results=1))
    return len(blobs) > 0


def verify_args(args: argparse.Namespace) -> None:
  """
  Verifies the command line arguments passed to the script.
  Raises an error if any of the arguments are invalid.
  """
  if args.totalPartitions <= 0:
    raise ValueError("Total number of partitions must be greater than 0.")

  if args.currPartition < 0 or args.currPartition >= args.totalPartitions:
    raise ValueError(
        f"Current partition number must be within the range [0, {args.totalPartitions})."
    )
  if args.maxStatVars is not None and args.maxStatVars <= 0:
    raise ValueError("maxStatVars must be a positive integer.")

  if args.runMode == config.RUN_MODE_RETRY_FAILURES:
    if not args.gcsFailureFolder:
      raise ValueError(
          "--gcsFailureFolder must be provided when runMode is 'retry_failures'."
      )
    if args.useGCS and not verify_gcs_path_exists(args.gcsFailureFolder):
      raise ValueError(
          f"GCS path {args.gcsFailureFolder} does not exist. Please check the path and try again."
      )
    elif not args.useGCS:
      raise ValueError(
          "--useGCS must be specified when runMode is 'retry_failures'.")

  if args.runMode == config.RUN_MODE_COMPACT and not args.gcsFolder:
    raise ValueError("Error: --gcs_folder is required for 'compact' mode.")


def get_language_settings(target_language: str) -> tuple[str, str]:
  match target_language:
    case "French":
      language_schema = json.dumps(frenchSchema)
    case "Spanish":
      language_schema = json.dumps(spanishSchema)
    case _:
      language_schema = json.dumps(englishSchema)

  output_file_name = f"{config.OUTPUT_FILENAME_PREFIX}_{target_language}"
  return output_file_name, get_gemini_prompt(language_schema)


def get_gcs_folder(gcs_folder: str | None, run_mode: str) -> str:
  """
  Returns the GCS folder to save the results to based on the user inputs.
  """
  # If in diffs or retry mode and a specific folder is given, use it directly.
  if (run_mode == config.RUN_MODE_BIGQUERY_DIFFS or
      run_mode == config.RUN_MODE_RETRY_FAILURES) and gcs_folder:
    return gcs_folder

  date_folder = datetime.now().strftime("%Y_%m_%d")

  if gcs_folder:
    # Store results in timestamp subfolders for other non-diff/non-retry runs.
    return os.path.join(gcs_folder, date_folder)

  if run_mode == config.RUN_MODE_RETRY_FAILURES:
    # Default location if no gcs_folder is provided
    return os.path.join(config.GCS_FILE_DIR_RETRIES, date_folder)

  if run_mode == config.RUN_MODE_BIGQUERY:
    return os.path.join(config.GCS_FILE_DIR_FULL, date_folder)

  return os.path.join(config.GCS_FILE_DIR_NL, date_folder)


def export_to_json(sv_metadata_list: list[dict[str, str | list[str]]],
                   exported_filename: str, should_save_to_gcs: bool,
                   gcs_folder: str | None) -> None:
  """
  Exports the SV metadata list to a JSON file.
  """
  if not sv_metadata_list:
    print(f"No StatVars to export for {exported_filename}. Skipping export.")
    return

  print(
      f"Exporting {len(sv_metadata_list)} StatVars to {exported_filename}.json..."
  )
  filename = f"{exported_filename}.json"
  local_file_path = os.path.join(config.EXPORTED_FILE_DIR, filename)
  try:
    sv_metadata_df = pd.DataFrame(sv_metadata_list)
  except OverflowError:
    print(
        "OverflowError encountered. Building DataFrame row by row and skipping bad rows."
    )
    good_rows = []
    bad_rows_count = 0
    for item in sv_metadata_list:
      try:
        # The error is with conversion, so we test if a DF can be made.
        pd.DataFrame([item])
        good_rows.append(item)
      except OverflowError:
        bad_rows_count += 1
        print(f"WARNING: Skipping a row due to OverflowError. Row data: {item}")

    if not good_rows:
      print("ERROR: All rows failed conversion. DataFrame will be empty.")
      sv_metadata_df = pd.DataFrame()
    else:
      sv_metadata_df = pd.DataFrame(good_rows)

    if bad_rows_count > 0:
      print(f"Skipped {bad_rows_count} rows due to OverflowError.")
  sv_metadata_json = sv_metadata_df.to_json(orient="records", lines=True)

  if should_save_to_gcs:
    gcs_client = storage.Client(project=config.GCS_PROJECT_ID)
    bucket = gcs_client.bucket(config.GCS_BUCKET)
    gcs_file_path = os.path.join(gcs_folder, filename)
    blob = bucket.blob(gcs_file_path)
    blob.upload_from_string(sv_metadata_json, content_type="application/json")

    print(
        f"{len(sv_metadata_df)} statvars saved to gs://{config.GCS_BUCKET}/{gcs_file_path}"
    )
  else:
    os.makedirs(os.path.join(config.EXPORTED_FILE_DIR, "failures"),
                exist_ok=True)
    with open(local_file_path, "w") as f:
      f.write(sv_metadata_json)
    print(f"{len(sv_metadata_df)} statvars saved to {local_file_path}")


def compact_files(gcs_folder: str, output_filename: str,
                  delete_originals: bool):
  """
  Compacts all .jsonl files in a GCS folder into a single file and optionally deletes the originals.
  """
  print(
      f"Starting compaction for GCS folder: gs://{config.GCS_BUCKET}/{gcs_folder}"
  )

  storage_client = storage.Client(project=config.GCS_PROJECT_ID)
  bucket = storage_client.bucket(config.GCS_BUCKET)

  if not output_filename:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f"compacted_{timestamp}.jsonl"

  # 1. List all original files to be compacted from the root of the folder
  # Ensure the prefix ends with a delimiter to correctly list folder contents.
  list_prefix = gcs_folder
  if not list_prefix.endswith('/'):
    list_prefix += '/'
  blobs_in_folder = bucket.list_blobs(prefix=list_prefix, delimiter='/')
  original_files = [
      blob for blob in blobs_in_folder
      if (blob.name.endswith('.jsonl') or blob.name.endswith('.json')) and
      output_filename not in blob.name and '/failures/' not in blob.name
  ]

  if not original_files:
    print("No .jsonl or .json files found to compact. Exiting.")
    return

  print(f"Found {len(original_files)} files to compact:")
  for blob in original_files:
    print(f"  - {blob.name}")

  # Use a temporary file to avoid holding all data in memory
  temp_file_path = "compacted_output.tmp.jsonl"

  # 2. Download and append all files to the temporary local file
  print("\nDownloading and merging files...")
  with open(temp_file_path, 'w', encoding='utf-8') as temp_f:
    for blob in original_files:
      print(f"  -> Merging {blob.name}")
      # Download content and write line by line to avoid extra newlines
      content = blob.download_as_text()
      for line in content.splitlines():
        if line.strip():
          temp_f.write(line + '\n')
  print("Merge complete.")

  # 3. Upload the new compacted file
  compacted_blob_path = os.path.join(gcs_folder, output_filename)
  compacted_blob = bucket.blob(compacted_blob_path)

  print(
      f"\nUploading new compacted file to: gs://{config.GCS_BUCKET}/{compacted_blob_path}"
  )
  compacted_blob.upload_from_filename(temp_file_path,
                                      content_type="application/json")
  print("Upload complete.")

  # 4. Clean up the local temporary file
  os.remove(temp_file_path)

  # 5. Delete original files if the flag is set
  if delete_originals:
    print(
        f"\n--delete_originals flag is set. Deleting {len(original_files)} original files..."
    )
    for blob in original_files:
      print(f"  -> Deleting {blob.name}")
      blob.delete()
    print("Deletion of original files complete.")
  else:
    print("\n--delete_originals flag not set. Original files have been kept.")

  print("\nCompaction process finished successfully.")


async def main():
  print("Starting NL metadata generation script...")
  args: argparse.Namespace = extract_flags()
  verify_args(args)

  sv_metadata_iter = None
  match args.runMode:
    case config.RUN_MODE_RETRY_FAILURES:
      sv_metadata_iter = data_loader.read_sv_metadata_failed_attempts(
          args.gcsFailureFolder, args.useGCS)
    case config.RUN_MODE_BIGQUERY:
      sv_metadata_iter = data_loader.create_sv_metadata_bigquery(
          args.totalPartitions, args.currPartition, args.maxStatVars)
    case config.RUN_MODE_NL_ONLY:
      sv_metadata_iter = [data_loader.create_sv_metadata_nl()]
    case config.RUN_MODE_BIGQUERY_DIFFS:
      sv_metadata_iter = data_loader.get_bigquery_diffs_metadata(
          args.gcsFolder, args.totalPartitions, args.currPartition,
          args.maxStatVars)
    case config.RUN_MODE_COMPACT:
      compact_files(args.gcsFolder, args.output_filename, args.delete_originals)
      return  # Exit after compaction
    case _:
      raise ValueError(f"Unknown run mode: {args.runMode}")

  target_language = args.language
  output_filename, gemini_prompt = get_language_settings(target_language)

  page_number: int = 1
  for sv_metadata_list in sv_metadata_iter:
    # When re-running failed attempts, sv_metadata_list already contains the full metadata.
    if args.runMode == config.RUN_MODE_RETRY_FAILURES:
      full_metadata = sv_metadata_list
    else:
      full_metadata: list[dict[str,
                               str | list[str]]] = data_loader.extract_metadata(
                                   sv_metadata_list, DC_API_KEY,
                                   args.runMode == config.RUN_MODE_BIGQUERY or
                                   args.runMode
                                   == config.RUN_MODE_BIGQUERY_DIFFS)
    failed_metadata: list[dict[str, str | list[str]]] = []

    # Generate the Alt Sentences using Gemini
    print(f"Generating alt sentences for batch number {page_number}")
    full_metadata, failed_metadata = await batch_generate_alt_sentences(
        full_metadata, args.geminiApiKey, gemini_prompt)

    gcs_folder = get_gcs_folder(args.gcsFolder,
                                args.runMode) if args.useGCS else None

    # Determine the folder for failure exports.
    gcs_failure_folder = gcs_folder
    if args.useGCS and args.gcsFailureFolder:
      gcs_failure_folder = args.gcsFailureFolder

    if args.runMode == config.RUN_MODE_BIGQUERY_DIFFS or args.runMode == config.RUN_MODE_RETRY_FAILURES:
      timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
      # Create a unique filename for the diff file from this job partition
      if args.runMode == config.RUN_MODE_RETRY_FAILURES:
        exported_filename = f"diff_retry_{timestamp}_{args.currPartition+1}"
        failed_filename = f"failures/failed_retry_{timestamp}_{args.currPartition+1}"
      else:  # bigquery_diffs
        exported_filename = f"diff_{timestamp}_{args.currPartition+1}"
        failed_filename = f"failures/failed_diff_{timestamp}_{args.currPartition+1}"
    else:
      exported_filename = f"{output_filename}_{args.currPartition+1}_{page_number}"
      failed_filename = f"failures/failed_batch_{args.currPartition+1}_{page_number}"

    export_to_json(full_metadata, exported_filename, args.useGCS, gcs_folder)
    export_to_json(failed_metadata, failed_filename, args.useGCS,
                   gcs_failure_folder)
    page_number += 1


if __name__ == "__main__":
  asyncio.run(main())
  print("NL metadata generation script finished.")
